import os
import openai
from typing import Optional, Dict, Any, Tuple
from vectorstore.chroma_db import chroma_manager
from utils.language_utils import language_handler

class EmbeddingAgent:
    def __init__(self):
        self.openai_client = openai.AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.similarity_threshold = 0.85  # Higher cosine similarity means better match
        self.high_similarity_threshold = 0.92  # Very high cosine similarity threshold
        
    async def process_message(self, user_message: str, conversation_history: list = None, user_language: str = 'ar') -> Dict[str, Any]:
        """
        Process incoming message by comparing to knowledge base using embeddings
        
        Returns:
        - action: 'reply', 'skip', or 'continue_to_classification'
        - response: the response text if action is 'reply'
        - confidence: confidence score
        - matched_question: the matched question from database
        """
        
        print(f"ğŸ” EmbeddingAgent: Processing message: '{user_message[:50]}...'")
        
        # Search for similar questions in the knowledge base
        search_results = chroma_manager.search(user_message, n_results=3)
        
        if not search_results:
            print(f"ğŸ“­ EmbeddingAgent: No knowledge base matches found")
            return {
                'action': 'continue_to_classification',
                'response': None,
                'confidence': 0,
                'matched_question': None
            }
        
        # Get the best match (highest cosine similarity)
        best_match = search_results[0]
        cosine_similarity = best_match.get('cosine_similarity', 0.0)  # Default to 0 if not found
        
        print(f"ğŸ¯ EmbeddingAgent: Best match found:")
        print(f"   - Question: {best_match['document'][:50]}...")
        print(f"   - Cosine Similarity: {cosine_similarity:.4f}")
        print(f"   - Metadata: {best_match['metadata']}")
        
        # Check if cosine similarity is good enough (higher is better)
        if cosine_similarity < self.similarity_threshold:
            print(f"âŒ EmbeddingAgent: Cosine similarity too low ({cosine_similarity:.4f} < {self.similarity_threshold})")
            return {
                'action': 'continue_to_classification',
                'response': None,
                'confidence': 0,
                'matched_question': None
            }
        
        # Get the corresponding answer
        matched_document = best_match['document']
        metadata = best_match['metadata']
        
        # If the matched document is a question, find its answer
        if metadata.get('type') == 'question':
            answer_id = metadata.get('answer_id')
            if answer_id:
                # Search for the answer by ID
                answer_results = chroma_manager.collection.get(ids=[answer_id])
                if answer_results and answer_results['documents']:
                    matched_answer = answer_results['documents'][0]
                else:
                    matched_answer = matched_document  # Fallback
            else:
                matched_answer = matched_document  # Fallback
        else:
            # The matched document is already an answer
            matched_answer = matched_document
        
        print(f"ğŸ“ EmbeddingAgent: Found answer: '{matched_answer[:50]}...'")
        
        # Check if the answer is empty or too short (likely needs no reply)
        if not matched_answer or matched_answer.strip() == "" or len(matched_answer.strip()) < 3:
            print(f"ğŸš« EmbeddingAgent: Empty or very short answer - no reply needed")
            return {
                'action': 'skip',
                'response': None,
                'confidence': cosine_similarity,
                'matched_question': matched_document
            }
        
        # For very high cosine similarity, skip the ChatGPT evaluation
        if cosine_similarity >= self.high_similarity_threshold:
            print(f"âœ… EmbeddingAgent: Very high cosine similarity ({cosine_similarity:.4f}) - using answer directly")
            return {
                'action': 'reply',
                'response': matched_answer,
                'confidence': cosine_similarity,
                'matched_question': matched_document
            }
        
        # Ask ChatGPT to evaluate if the response is appropriate
        evaluation_result = await self._evaluate_response_with_chatgpt(
            user_message, matched_document, matched_answer, user_language, conversation_history
        )
        
        print(f"ğŸ¤– EmbeddingAgent: ChatGPT evaluation: {evaluation_result}")
        
        if evaluation_result['action'] == 'reply':
            return {
                'action': 'reply',
                'response': evaluation_result.get('response', matched_answer),
                'confidence': cosine_similarity,
                'matched_question': matched_document
            }
        elif evaluation_result['action'] == 'skip':
            return {
                'action': 'skip',
                'response': None,
                'confidence': cosine_similarity,
                'matched_question': matched_document
            }
        else:
            return {
                'action': 'continue_to_classification',
                'response': None,
                'confidence': cosine_similarity,
                'matched_question': matched_document
            }
    
    async def _evaluate_response_with_chatgpt(self, user_message: str, matched_question: str, 
                                            matched_answer: str, language: str, conversation_history: list = None) -> Dict[str, Any]:
        """
        Ask ChatGPT to evaluate if the response is good and appropriate
        """
        
        # Format conversation history for context
        conversation_context = ""
        if conversation_history:
            # Get the latest 3 messages for context
            recent_messages = conversation_history[-3:] if len(conversation_history) >= 3 else conversation_history
            
            if language == 'ar':
                conversation_context = "\n\nØ³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© (Ø¢Ø®Ø± 3 Ø±Ø³Ø§Ø¦Ù„):\n"
                for i, msg in enumerate(recent_messages, 1):
                    role = "Ø§Ù„Ø¹Ù…ÙŠÙ„" if msg.get('role') == 'user' else "Ø§Ù„ÙˆÙƒÙŠÙ„"
                    conversation_context += f"{i}. {role}: {msg.get('content', '')}\n"
            else:
                conversation_context = "\n\nConversation context (last 3 messages):\n"
                for i, msg in enumerate(recent_messages, 1):
                    role = "Customer" if msg.get('role') == 'user' else "Agent"
                    conversation_context += f"{i}. {role}: {msg.get('content', '')}\n"
        
        if language == 'ar':
            evaluation_prompt = f"""Ø£Ù†Øª Ù…Ù‚ÙŠÙ… Ø°ÙƒÙŠ Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø±Ø¯ÙˆØ¯ ÙÙŠ Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ù„Ø´Ø±ÙƒØ© Ø£Ø¨Ø§Ø± Ù„ØªÙˆØµÙŠÙ„ Ø§Ù„Ù…ÙŠØ§Ù‡.

Ø³Ø£Ø¹Ø·ÙŠÙƒ:
1. Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠØ©
2. Ø³Ø¤Ø§Ù„ Ù…Ø´Ø§Ø¨Ù‡ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
3. Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ø­ÙÙˆØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
4. Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù„Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©{conversation_context}

Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠØ©: "{user_message}"
Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: "{matched_question}"
Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ø­ÙÙˆØ¸: "{matched_answer}"

Ù…Ù‡Ù…ØªÙƒ ØªÙ‚ÙŠÙŠÙ… Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ø­ÙÙˆØ¸ Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ø£Ù… Ù„Ø§ØŒ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.

Ù‚Ù… Ø¨Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ÙˆØ§Ø®ØªØ± Ø¥Ø­Ø¯Ù‰ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:

1. "reply" - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø¯ Ù…Ù†Ø§Ø³Ø¨ ÙˆÙ…ÙÙŠØ¯ ÙˆÙŠØ¬ÙŠØ¨ Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¹Ù…ÙŠÙ„
2. "skip" - Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø±Ø¯ (Ù…Ø«Ù„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±ØŒ "Ø£ÙˆÙƒÙŠ", "Ø´ÙƒØ±Ø§Ù‹", "ØªÙ…Ø§Ù…")
3. "continue" - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø¯ ØºÙŠØ± Ù…Ù†Ø§Ø³Ø¨ Ø£Ùˆ Ø§Ù„Ø±Ø³Ø§Ù„Ø© ØªØ­ØªØ§Ø¬ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ø§Ù‹

Ø§ÙƒØªØ¨ ÙÙ‚Ø· Ø§Ù„ÙƒÙ„Ù…Ø©: reply Ø£Ùˆ skip Ø£Ùˆ continue"""
        else:
            evaluation_prompt = f"""You are a smart response quality evaluator for Abar water delivery customer service.

I will give you:
1. Current customer message
2. Similar question from database
3. Stored response from database
4. Conversation context from previous messages{conversation_context}

Current customer message: "{user_message}"
Similar question from database: "{matched_question}"
Stored response: "{matched_answer}"

Your task is to evaluate whether the stored response is appropriate for the current message, considering the conversation context.

Evaluate and choose one of the following options:

1. "reply" - if the response is appropriate, helpful and answers the customer's question
2. "skip" - if the customer message doesn't need a reply (like emotions, "ok", "thanks", "fine")
3. "continue" - if the response is not appropriate or the message needs more complex processing

Write only the word: reply or skip or continue"""
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "user", "content": evaluation_prompt}
                ],
                max_tokens=10,
                temperature=0.1
            )
            
            evaluation = response.choices[0].message.content.strip().lower()
            
            # Map the response to our action format
            if 'reply' in evaluation:
                return {'action': 'reply'}
            elif 'skip' in evaluation:
                return {'action': 'skip'}
            elif 'continue' in evaluation:
                return {'action': 'continue'}
            else:
                # Default to continue if we can't parse the response
                return {'action': 'continue'}
                
        except Exception as e:
            print(f"âŒ EmbeddingAgent: Error evaluating response with ChatGPT: {str(e)}")
            # Default to continue on error
            return {'action': 'continue'}

# Create and export the instance
embedding_agent = EmbeddingAgent() 