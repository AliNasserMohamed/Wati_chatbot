import os
import openai
import time
from typing import Optional, Dict, Any, Tuple
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from vectorstore.chroma_db import chroma_manager
from utils.language_utils import language_handler

# Import message journey logger for detailed logging
try:
    from utils.message_logger import message_journey_logger
    LOGGING_AVAILABLE = True
except ImportError:
    LOGGING_AVAILABLE = False
    print("‚ö†Ô∏è Message journey logger not available - detailed embedding logging disabled")

class EmbeddingAgent:
    def __init__(self):
        # Keep AsyncOpenAI for fallback if needed
        self.openai_client = openai.AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        # Initialize LangChain client for better tracing
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.1,
            api_key=os.getenv("OPENAI_API_KEY"),
            tags=["embedding-agent", "abar-chatbot"]
        )
        self.similarity_threshold = 0.50  # Higher cosine similarity means better match
        
    async def process_message(self, user_message: str, conversation_history: list = None, user_language: str = 'ar', journey_id: str = None) -> Dict[str, Any]:
        """
        Process incoming message by comparing to knowledge base using embeddings
        
        Returns:
        - action: 'reply', 'skip', or 'continue_to_classification'
        - response: the response text if action is 'reply'
        - confidence: confidence score
        - matched_question: the matched question from database
        """
        
        print(f"üîç EmbeddingAgent: Processing message: '{user_message[:50]}...'")
        
        # Detect the actual language of the user message
        detected_user_language = language_handler.detect_language(user_message)
        print(f"üåê EmbeddingAgent: User message language detected as: {detected_user_language}")
        
        # Search for similar questions in the knowledge base
        search_results = await chroma_manager.search(user_message, n_results=3)
        
        if not search_results:
            print(f"üì≠ EmbeddingAgent: No knowledge base matches found")
            return {
                'action': 'continue_to_classification',
                'response': None,
                'confidence': 0,
                'matched_question': None
            }
        
        # Print detailed information about all search results
        print(f"üìä EmbeddingAgent: Found {len(search_results)} similar results:")
        
        # Detailed logging: capture all search results for journey logging
        search_results_for_log = []
        
        for i, result in enumerate(search_results, 1):
            similarity_score = result.get('similarity', result.get('distance', 0.0))
            document_preview = result['document'][:100] + "..." if len(result['document']) > 100 else result['document']
            metadata = result.get('metadata', {})
            
            print(f"   Result {i}:")
            print(f"   - Similarity Score: {similarity_score:.4f}")
            print(f"   - Document Type: {metadata.get('type', 'unknown')}")
            print(f"   - Content Preview: {document_preview}")
            print(f"   - Has Answer Text: {bool(metadata.get('answer_text', '').strip())}")
            print(f"   - Full Metadata: {metadata}")
            print(f"   ---")
            
            # Additional debugging: Check if this is a question without an answer
            if metadata.get('type') == 'question':
                answer_text = metadata.get('answer_text', '')
                if not answer_text or not answer_text.strip():
                    print(f"   ‚ö†Ô∏è  WARNING: Question without answer text!")
                elif metadata.get('has_answer') == False:
                    print(f"   ‚ÑπÔ∏è  Info: Question marked as having no answer")
            
            # Capture for detailed logging
            search_results_for_log.append({
                "rank": i,
                "similarity_score": similarity_score,
                "document": result['document'],
                "document_type": metadata.get('type', 'unknown'),
                "has_answer_text": bool(metadata.get('answer_text', '').strip()),
                "has_answer": metadata.get('has_answer'),
                "metadata": metadata
            })
        
        # Log all search results to journey if logging is available
        if LOGGING_AVAILABLE and journey_id:
            message_journey_logger.add_step(
                journey_id=journey_id,
                step_type="embedding_search_results",
                description=f"Found {len(search_results)} similar questions in knowledge base",
                data={
                    "search_results": search_results_for_log,
                    "user_message": user_message,
                    "top_similarity": search_results_for_log[0]["similarity_score"] if search_results_for_log else 0
                }
            )
        
        # Get the best match (highest similarity)
        best_match = search_results[0]
        similarity_score = best_match.get('similarity', 0.0)  # Use 'similarity' not 'cosine_similarity'
        
        print(f"üéØ EmbeddingAgent: Best match selected:")
        print(f"   - Question: {best_match['document'][:100]}...")
        print(f"   - Similarity: {similarity_score:.4f}")
        print(f"   - Type: {best_match.get('metadata', {}).get('type', 'unknown')}")
        print(f"   - Metadata: {best_match['metadata']}")
        print(f"   - Has Answer Text: {bool(best_match.get('metadata', {}).get('answer_text', '').strip())}")
        
        # Check if similarity is good enough (higher is better)
        if similarity_score < self.similarity_threshold:
            print(f"‚ùå EmbeddingAgent: Similarity too low ({similarity_score:.4f} < {self.similarity_threshold})")
            return {
                'action': 'continue_to_classification',
                'response': None,
                'confidence': 0,
                'matched_question': None
            }
        
        # Get the corresponding answer
        matched_document = best_match['document']
        metadata = best_match['metadata']
        
        print(f"üîç EmbeddingAgent: Processing matched result...")
        print(f"   - Matched Document: {matched_document[:100]}...")
        print(f"   - Document Type: {metadata.get('type', 'unknown')}")
        print(f"   - Metadata: {metadata}")
        
        # CRITICAL: Always get the answer, never return the question
        final_answer = None
        matched_question_text = None
        
        # If the matched document is a question, get its answer from metadata
        if metadata.get('type') == 'question':
            matched_question_text = matched_document
            answer_text = metadata.get('answer_text', '')  # Get answer from metadata
            
            print(f"   - This is a QUESTION: '{matched_question_text}...'")
            print(f"   - Answer from metadata: '{answer_text if answer_text else 'No answer'}...'")
            
            if answer_text and answer_text.strip():
                final_answer = answer_text.strip()
                print(f"   - ‚úÖ Found ANSWER in metadata: '{final_answer}...'")
                
                # Log the matched question-answer pair for journey tracking
                if LOGGING_AVAILABLE and journey_id:
                    message_journey_logger.add_step(
                        journey_id=journey_id,
                        step_type="embedding_qa_match",
                        description="Successfully matched question with answer from knowledge base",
                        data={
                            "matched_question": matched_question_text,
                            "matched_answer": final_answer,
                            "similarity_score": similarity_score,
                            "answer_source": "metadata",
                            "question_metadata": metadata
                        }
                    )
            else:
                print(f"   - ‚ùå No answer found in question metadata")
                final_answer = None
                
        else:
            # The matched document is already an answer (shouldn't happen with new approach)
            print(f"   - ‚ö†Ô∏è  Matched document appears to be an answer directly (unexpected with new approach)")
            final_answer = matched_document
            matched_question_text = "Direct answer match"

        
        # CRITICAL: If no valid answer found, don't reply
        if not final_answer or final_answer.strip() == "":
            print(f"üö´ EmbeddingAgent: No valid answer found - skipping reply")
            print(f"   - Question: {matched_question_text}")
            print(f"   - Answer: '{final_answer}'")
            return {
                'action': 'skip',
                'response': None,
                'confidence': similarity_score,
                'matched_question': matched_question_text or matched_document,
                'error': 'No valid answer found'
            }
        
        # Check if answer is too short
        if len(final_answer.strip()) < 2:
            print(f"üö´ EmbeddingAgent: Answer too short - skipping reply")
            print(f"   - Answer: '{final_answer}'")
            return {
                'action': 'skip',
                'response': None,
                'confidence': similarity_score,
                'matched_question': matched_question_text or matched_document,
                'error': 'Answer too short'
            }
        
        print(f"üìù EmbeddingAgent: Valid answer found!")
        print(f"   - Question: {matched_question_text[:50]}...")
        print(f"   - Answer: {final_answer[:100]}...")
        print(f"   - Answer Length: {len(final_answer)} characters")
        
        # CRITICAL: Check language matching BEFORE processing
        user_language = language_handler.detect_language(user_message)
        answer_language = language_handler.detect_language(final_answer)
        
        print(f"üåê Language Check:")
        print(f"   - User message language: {user_language}")
        print(f"   - Answer language: {answer_language}")
        
        # If languages don't match, proceed to classification
        if user_language != answer_language:
            print(f"üîÑ Language mismatch: user={user_language}, answer={answer_language} - proceeding to classification")
            return {
                'action': 'continue_to_classification',
                'response': None,
                'confidence': similarity_score,
                'matched_question': matched_question_text or matched_document,
                'error': f'Language mismatch: user={user_language}, answer={answer_language}'
            }
        
       
        
        # Ask ChatGPT to evaluate if the response is appropriate
        evaluation_result = await self._evaluate_response_with_chatgpt(
            user_message, matched_question_text or matched_document, final_answer, detected_user_language, conversation_history, journey_id
        )
        
        print(f"ü§ñ EmbeddingAgent: ChatGPT evaluation: {evaluation_result}")
        
        if evaluation_result['action'] == 'reply':
            return {
                'action': 'reply',
                'response': evaluation_result.get('response', final_answer),
                'confidence': similarity_score,
                'matched_question': matched_question_text or matched_document
            }
        elif evaluation_result['action'] == 'skip':
            return {
                'action': 'skip',
                'response': None,
                'confidence': similarity_score,
                'matched_question': matched_question_text or matched_document
            }
        else:
            return {
                'action': 'continue_to_classification',
                'response': None,
                'confidence': similarity_score,
                'matched_question': matched_question_text or matched_document
            }
    
    async def _evaluate_response_with_chatgpt(self, user_message: str, matched_question: str, 
                                            matched_answer: str, language: str, conversation_history: list = None, journey_id: str = None) -> Dict[str, Any]:
        """
        Ask ChatGPT to evaluate if the response is good and appropriate
        """
        
        # First check if the user message and matched answer are in the same language
        user_language = language_handler.detect_language(user_message)
        answer_language = language_handler.detect_language(matched_answer)
        
        # If languages don't match, proceed to classification
        if user_language != answer_language:
            print(f"üåê Language mismatch: user={user_language}, answer={answer_language} - proceeding to classification")
            return {'action': 'continue'}
        
        # Format conversation history for context - enhanced for better decision making
        conversation_context = ""
        history_analysis = ""
        
        if conversation_history and len(conversation_history) > 0:
            # Get more messages for better context (up to 10 messages)
            recent_messages = conversation_history[-10:] if len(conversation_history) >= 10 else conversation_history
            
            if language == 'ar':
                conversation_context = f"\n\nÿ≥ŸäÿßŸÇ ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ© (ÿ¢ÿÆÿ± {len(recent_messages)} ÿ±ÿ≥ÿßÿ¶ŸÑ):\n"
                for i, msg in enumerate(recent_messages, 1):
                    role = "ÿßŸÑÿπŸÖŸäŸÑ" if msg.get('role') == 'user' else "ÿßŸÑŸàŸÉŸäŸÑ"
                    conversation_context += f"{i}. {role}: {msg.get('content', '')}\n"
                    
                # Add analysis instructions
                history_analysis = f"\n\nÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®:\n- ÿ±ÿßÿ¨ÿπ ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ© ÿ®ÿπŸÜÿßŸäÿ© ŸÑŸÅŸáŸÖ ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÉÿßŸÖŸÑ\n- ÿßŸÜÿ™ÿ®Ÿá ÿ•ŸÑŸâ ŸÖÿß ÿ™ŸÖ ŸÖŸÜÿßŸÇÿ¥ÿ™Ÿá ÿ≥ÿßÿ®ŸÇÿßŸã ÿ®ŸäŸÜ ÿßŸÑÿπŸÖŸäŸÑ ŸàÿßŸÑŸàŸÉŸäŸÑ\n- ÿßÿπÿ™ÿ®ÿ± ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿßŸÑÿ±ÿ≥ÿßŸÑÿ© ÿßŸÑÿ≠ÿßŸÑŸäÿ© ŸÖÿ±ÿ™ÿ®ÿ∑ÿ© ÿ®ŸÖŸàÿ∂Ÿàÿπ ÿ≥ÿßÿ®ŸÇ ŸÅŸä ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ©\n- ÿßÿπÿ™ÿ®ÿ± ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑÿπŸÖŸäŸÑ Ÿäÿ™ÿßÿ®ÿπ ÿ≥ÿ§ÿßŸÑ ÿ≥ÿßÿ®ŸÇ ÿ£Ÿà Ÿäÿ∑ŸÑÿ® ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿ•ÿ∂ÿßŸÅŸäÿ©\n- ÿßÿπÿ™ÿ®ÿ± ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑŸàŸÉŸäŸÑ ŸÇÿØ ÿ£ÿ¨ÿßÿ® ÿπŸÑŸâ ÿ≥ÿ§ÿßŸÑ ŸÖÿ¥ÿßÿ®Ÿá ŸÖŸÜ ŸÇÿ®ŸÑ\n"
            else:
                conversation_context = f"\n\nConversation context (last {len(recent_messages)} messages):\n"
                for i, msg in enumerate(recent_messages, 1):
                    role = "Customer" if msg.get('role') == 'user' else "Agent"
                    conversation_context += f"{i}. {role}: {msg.get('content', '')}\n"
                    
                # Add analysis instructions
                history_analysis = f"\n\nContext analysis required:\n- Carefully review the conversation to understand the complete context\n- Pay attention to what has been discussed previously between customer and agent\n- Consider if the current message relates to a previous topic in the conversation\n- Consider if the customer is following up on a previous question or asking for additional information\n- Consider if the agent has already answered a similar question before\n"
        else:
            if language == 'ar':
                conversation_context = "\n\nŸÑÿß ŸäŸàÿ¨ÿØ ÿ≥ŸäÿßŸÇ ŸÖÿ≠ÿßÿØÿ´ÿ© ÿ≥ÿßÿ®ŸÇ.\n"
            else:
                conversation_context = "\n\nNo previous conversation context available.\n"
        
        if language == 'ar':
            evaluation_prompt = f"""ÿ£ŸÜÿ™ ŸÖŸÇŸäŸÖ ÿµÿßÿ±ŸÖ ÿ¨ÿØÿßŸã ŸÑÿ¨ŸàÿØÿ© ÿßŸÑÿ±ÿØŸàÿØ ŸÅŸä ÿÆÿØŸÖÿ© ÿßŸÑÿπŸÖŸÑÿßÿ° ŸÑÿ¥ÿ±ŸÉÿ© ÿ£ÿ®ÿßÿ± ŸÑÿ™ŸàÿµŸäŸÑ ÿßŸÑŸÖŸäÿßŸá.

ŸÖŸáŸÖÿ™ŸÉ ÿßŸÑŸàÿ≠ŸäÿØÿ©: ÿ™ÿµŸÜŸäŸÅ ÿ±ÿ≥ÿßŸÑÿ© ÿßŸÑÿπŸÖŸäŸÑ ÿ®ÿØŸÇÿ© ÿ•ŸÑŸâ Ÿàÿßÿ≠ÿØÿ© ŸÖŸÜ ÿ´ŸÑÿßÿ´ ÿ≠ÿßŸÑÿßÿ™ ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑÿ±ÿ≥ÿßŸÑÿ© ÿßŸÑÿ≠ÿßŸÑŸäÿ© ŸàÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑÿ™ÿßÿ±ŸäÿÆŸä ŸÑŸÑŸÖÿ≠ÿßÿØÿ´ÿ©.

- ÿ±ÿ≥ÿßŸÑÿ© ÿßŸÑÿπŸÖŸäŸÑ ÿßŸÑÿ≠ÿßŸÑŸäÿ©: "{user_message}"
- ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿßŸÑŸÖÿ¥ÿßÿ®Ÿá ŸÖŸÜ ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ : "{matched_question}"
- ÿßŸÑÿ±ÿØ ÿßŸÑŸÖÿ≠ŸÅŸàÿ∏: "{matched_answer}"

{conversation_context}

{history_analysis}

üö® ÿ™ÿπŸÑŸäŸÖÿßÿ™ ŸÖŸáŸÖÿ© ÿ¨ÿØÿßŸã ÿ≠ŸàŸÑ ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿ≥ŸäÿßŸÇ ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ©:
- ÿ±ÿßÿ¨ÿπ ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ© ÿßŸÑÿ≥ÿßÿ®ŸÇÿ© ÿ®ÿπŸÜÿßŸäÿ© ŸÅÿßÿ¶ŸÇÿ© ŸÇÿ®ŸÑ ÿßÿ™ÿÆÿßÿ∞ ÿßŸÑŸÇÿ±ÿßÿ±
- ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑÿπŸÖŸäŸÑ ŸÇÿØ ÿ≥ÿ£ŸÑ ÿ≥ÿ§ÿßŸÑÿßŸã ÿ≥ÿßÿ®ŸÇÿßŸã Ÿàÿ£ÿ¨ÿ®ŸÜÿß ÿπŸÑŸäŸáÿå ŸàŸÑŸÉŸÜŸá ŸäÿπŸäÿØ ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿ®ÿ∑ÿ±ŸäŸÇÿ© ŸÖÿÆÿ™ŸÑŸÅÿ©ÿå ŸÅŸÉÿ± ŸÅŸä ÿßŸÑÿ≥ŸäÿßŸÇ
- ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑÿπŸÖŸäŸÑ Ÿäÿ™ÿßÿ®ÿπ ŸÖŸàÿ∂Ÿàÿπ ÿ≥ÿßÿ®ŸÇÿå ÿßÿπÿ™ÿ®ÿ± ÿ∞ŸÑŸÉ ŸÅŸä ÿßŸÑŸÇÿ±ÿßÿ±
- ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑŸàŸÉŸäŸÑ ŸÇÿØ ÿ£ÿ±ÿ≥ŸÑ ÿ±ÿØÿßŸã ŸÖÿπŸäŸÜÿßŸã ŸÖŸÜ ŸÇÿ®ŸÑÿå ŸÑÿß ÿ™ŸÉÿ±ÿ±Ÿá ÿ•ŸÑÿß ÿ•ÿ∞ÿß ŸÉÿßŸÜ ŸÖŸÜÿßÿ≥ÿ®ÿßŸã ÿ≠ŸÇÿßŸã
- ÿßŸÜÿ™ÿ®Ÿá ŸÑÿ™ÿØŸÅŸÇ ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ© ŸàÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑÿπÿßŸÖ

ÿßŸÑÿ™ÿµŸÜŸäŸÅ Ÿäÿ¨ÿ® ÿ£ŸÜ Ÿäÿπÿ™ŸÖÿØ ÿπŸÑŸâ ÿßŸÑŸÇŸàÿßÿπÿØ ÿßŸÑÿ™ÿßŸÑŸäÿ© (ŸÖÿπ ŸÖÿ±ÿßÿπÿßÿ© ÿ≥ŸäÿßŸÇ ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ©):

üü¢ "reply":
-‚úÖreply  ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿ±ÿ≥ÿßŸÑÿ© ÿßŸÑÿπŸÖŸäŸÑ ÿßŸÑÿ≠ÿßŸÑŸäÿ© ŸÖÿ¥ÿßÿ®Ÿáÿ© ŸÑÿ≥ÿ§ÿßŸÑ ŸÖŸàÿ¨ŸàÿØ ŸÅŸä ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿå ŸàŸÉÿßŸÜ ŸÑÿØŸäŸÜÿß ÿ±ÿØ ŸÖÿ≠ŸÅŸàÿ∏ ŸÑŸá ‚Äî ÿ≥Ÿàÿßÿ° ŸÉÿßŸÜÿ™ ÿ™ÿ≠Ÿäÿ© ÿ£Ÿà ÿßÿ≥ÿ™ŸÅÿ≥ÿßÿ± ÿ£Ÿà ÿ∑ŸÑÿ® ‚Äî Ÿäÿ¨ÿ® ÿßÿÆÿ™Ÿäÿßÿ± 

- ÿ£Ÿà ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿßŸÑÿ±ÿ≥ÿßŸÑÿ© ŸÖÿ¨ÿ±ÿØ ÿ™ÿ≠Ÿäÿ© ÿ£Ÿà ÿ¥ŸÉÿ± ÿ®ÿ≥Ÿäÿ∑ ÿ®ÿØŸàŸÜ ÿ£Ÿä ŸÖÿ≠ÿ™ŸàŸâ ÿ•ÿ∂ÿßŸÅŸä (ŸÖÿπ ÿ£Ÿà ÿ®ÿØŸàŸÜ ÿπŸÑÿßŸÖÿßÿ™ ÿ™ÿ±ŸÇŸäŸÖ ŸÖÿ´ŸÑ ÿßŸÑŸÜŸÇÿßÿ∑ ÿ£Ÿà ÿßŸÑŸÖÿ≥ÿßŸÅÿßÿ™)
  - ŸÖÿ´ŸÑ: (ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖÿå ŸÖÿ±ÿ≠ÿ®ÿßÿå ÿ£ŸáŸÑÿßŸãÿå ŸáŸÑÿßÿå ŸáŸÑÿß Ÿàÿ∫ŸÑÿßÿå ÿµÿ®ÿßÿ≠ ÿßŸÑÿÆŸäÿ±ÿå ŸÖÿ≥ÿßÿ° ÿßŸÑÿÆŸäÿ±ÿå ÿ¥ŸÉÿ±ÿßŸãÿå Ÿäÿπÿ∑ŸäŸÉ ÿßŸÑÿπÿßŸÅŸäÿ©ÿå ÿ¨ÿ≤ÿßŸÉ ÿßŸÑŸÑŸá ÿÆŸäÿ±ÿå ÿßŸÑŸÑŸá ŸäŸàŸÅŸÇŸÉŸÖÿå ÿ¥ŸÉÿ±ÿß ŸÑŸÉÿå ŸÖÿ¥ŸÉŸàÿ±)
  - ÿ£Ÿà ŸÜŸÅÿ≥ ÿßŸÑÿ™ÿ≠Ÿäÿßÿ™ ŸÖÿπ ÿπŸÑÿßŸÖÿßÿ™ ÿ™ÿ±ŸÇŸäŸÖ ŸÖÿ´ŸÑ: (ŸáŸÑÿß...ÿå ŸÖÿ±ÿ≠ÿ®ÿß.ÿå ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ!!ÿå ÿ¥ŸÉÿ±ÿß...) Ÿäÿ¨ÿ® ÿßÿÆÿ™Ÿäÿßÿ± reply

- ŸÑŸÉŸÜ ÿ™ÿ£ŸÉÿØ ÿ£ŸàŸÑÿßŸã ŸÖŸÜ ÿ≥ŸäÿßŸÇ ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ©: ÿ•ÿ∞ÿß ŸÉÿßŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ±ÿØ ŸÇÿØ ÿ£Ÿèÿ±ÿ≥ŸÑ ŸÖÿ§ÿÆÿ±ÿßŸãÿå ŸÇÿØ ÿ™ÿ≠ÿ™ÿßÿ¨ ŸÑÿßÿÆÿ™Ÿäÿßÿ± "continue" ÿ®ÿØŸÑÿßŸã ŸÖŸÜ ÿ∞ŸÑŸÉ

üü° "skip":
- ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿßŸÑÿ±ÿ≥ÿßŸÑÿ© ŸÇÿµŸäÿ±ÿ© ŸàŸÑÿß ÿ™ÿ™ÿ∑ŸÑÿ® ÿ±ÿØ ŸÖÿ´ŸÑ: (ÿ™ŸÖÿßŸÖÿå ÿ∑Ÿäÿ®ÿå ÿ£ŸàŸÉÿå ÿ£ŸàŸÉŸäÿå ÿ™ŸÖÿßŸÖ ÿßŸÑÿ™ŸÖÿßŸÖÿå ÿÆŸÑÿßÿµ)
- ÿ£Ÿà ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑÿπŸÖŸäŸÑ Ÿäÿ§ŸÉÿØ ŸÅŸáŸÖŸá ŸÑÿ¥Ÿäÿ° ÿ™ŸÖ ÿ¥ÿ±ÿ≠Ÿá ŸÑŸá ŸÅŸä ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ©

üî¥ "continue":
- ÿ•ÿ∞ÿß ŸÑŸÖ ÿ™ŸÉŸÜ ÿ™ÿ≠Ÿäÿ© ÿ£Ÿà ÿ¥ŸÉÿ± ÿ®ÿ≥Ÿäÿ∑
- ŸàŸÑŸÖ ŸÜÿ¨ÿØ ŸÑŸáÿß ÿ™ÿ∑ÿßÿ®ŸÇŸãÿß Ÿàÿßÿ∂ÿ≠Ÿãÿß ŸÅŸä ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ (ÿ£Ÿä ŸÑŸÖ ÿ™ŸÉŸÜ ŸÖÿ¥ÿßÿ®Ÿáÿ© ŸÑÿ≥ÿ§ÿßŸÑ ŸÖŸàÿ¨ŸàÿØ ŸÑÿØŸäŸÜÿß)
- ÿ£Ÿà ŸÉÿßŸÜÿ™ ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿ™ÿ≠Ÿäÿ© ÿ£Ÿà ÿ¥ŸÉÿ± ŸÑŸÉŸÜ ŸÖÿ±ŸÅŸÇÿ© ÿ®ÿ≥ÿ§ÿßŸÑ ÿ£Ÿà ÿ∑ŸÑÿ®
- ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑÿ≥ŸäÿßŸÇ Ÿäÿ¥Ÿäÿ± ÿ•ŸÑŸâ ÿ£ŸÜ ÿßŸÑÿπŸÖŸäŸÑ Ÿäÿ≠ÿ™ÿßÿ¨ ŸÑŸÖÿ≥ÿßÿπÿØÿ© ÿ£ŸÉÿ´ÿ± ÿ™ÿÆÿµÿµÿßŸã
- ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑÿπŸÖŸäŸÑ Ÿäÿ™ÿßÿ®ÿπ ŸÖŸàÿ∂ŸàÿπÿßŸã ŸÑŸÖ ŸÜÿ≠ŸÑŸá ÿ®ÿßŸÑŸÉÿßŸÖŸÑ ŸÅŸä ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ© ÿßŸÑÿ≥ÿßÿ®ŸÇÿ©

‚ùóÔ∏èŸÖŸÑÿ≠Ÿàÿ∏ÿ© ŸÖŸáŸÖÿ© ÿ¨ÿØÿßŸã (ŸÖÿπ ŸÖÿ±ÿßÿπÿßÿ© ÿßŸÑÿ≥ŸäÿßŸÇ):
- ÿ•ÿ∞ÿß ŸàŸèÿ¨ÿØ ÿ™ÿ∑ÿßÿ®ŸÇ ŸÅŸä ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸàŸÉÿßŸÜ ŸáŸÜÿßŸÉ ÿ±ÿØ ŸÖÿ≠ŸÅŸàÿ∏ÿå ÿßÿÆÿ™ÿ± "reply" - ÿ•ŸÑÿß ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑÿ≥ŸäÿßŸÇ Ÿäÿ¥Ÿäÿ± ŸÑÿπŸÉÿ≥ ÿ∞ŸÑŸÉ
- ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿßŸÑÿ±ÿ≥ÿßŸÑÿ© ŸÅŸÇÿ∑ "ÿ¥ŸÉÿ±ÿßŸã" ÿ£Ÿà "ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ"ÿå ÿßÿÆÿ™ÿ± "reply" - ÿ•ŸÑÿß ÿ•ÿ∞ÿß ŸÉÿßŸÜ ŸÖŸÉÿ±ÿ±ÿßŸã ŸÅŸä ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ©
- ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ŸÖÿ´ŸÑ "ÿ™ŸÖÿßŸÖ" ÿ£Ÿà "ÿ£ŸàŸÉ"ÿå ÿßÿÆÿ™ÿ± "skip"
- ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿ≥ÿ§ÿßŸÑ ÿ£Ÿà ÿßÿ≥ÿ™ŸÅÿ≥ÿßÿ± ŸàŸÑŸÖ ŸÜÿ¨ÿØ ŸÑŸáÿß ÿ™ÿ∑ÿßÿ®ŸÇÿßŸã ŸÅŸä ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ÿå ÿßÿÆÿ™ÿ± "continue"

‚ùóÔ∏èŸÖŸÑÿ≠Ÿàÿ∏Ÿá ŸÖŸáŸÖÿ© ÿ¨ÿØÿß ÿ¨ÿØÿß (ŸÖÿπ ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ≥ŸäÿßŸÇ):
- ÿßÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿπŸÜ ŸÖÿßÿ±ŸÉÿ© ŸÖŸäÿßŸá ŸÖÿπŸäŸÜÿ© ÿßŸà ŸÖŸÜÿ™ÿ¨ ŸÖÿπŸäŸÜ ÿßŸà ÿ≥ÿπÿ± ŸÖŸÜÿ™ÿ¨ ŸÖÿπŸäŸÜ ÿßŸà ÿßŸÑÿ≥ŸàÿßŸÑ ÿπŸÜ ÿßŸÑÿßÿ≥ÿπÿßÿ± ÿßŸà ÿßŸÑŸÖÿßÿ±ŸÉÿßÿ™ ŸÅŸä ŸÖÿØŸäŸÜÿ© ŸÖÿπŸäŸÜÿ© ÿßÿÆÿ™ÿ± "continue"
- ÿ•ÿ∞ÿß ÿ£ÿÆÿ®ÿ±ŸÜÿß ÿßŸÑÿπŸÖŸäŸÑ ÿ®Ÿáÿ∞ÿß ÿßŸÑÿ±ÿØ ŸÖÿ≥ÿ®ŸÇÿßŸã "ÿ®ÿ™ÿ≠ÿµŸÑ ÿßŸÑÿßÿµŸÜÿßŸÅ ŸàÿßŸÑÿßÿ≥ÿπÿßÿ± ŸÅŸä ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇ ŸàŸáÿ∞ÿß ŸáŸà ÿßŸÑÿ±ÿßÿ®ÿ∑ https://onelink.to/abar_app https://abar.app/en/store/ ŸàÿßŸäÿ∂ÿß ÿπŸÜ ÿ∑ÿ±ŸäŸÇ ÿßŸÑŸÖŸàŸÇÿπ ÿßŸÑÿßŸÑŸÉÿ™ÿ±ŸàŸÜŸä"
ŸÅŸÑÿß Ÿäÿ¨ÿ® ÿßŸÑÿ±ÿØ ÿ®Ÿáÿ∞ÿß ÿßŸÑÿ±ÿØ ŸÖÿ±ÿ© ÿ´ÿßŸÜŸäÿ© ŸàŸäÿ¨ÿ® ÿßÿÆÿ™Ÿäÿßÿ± continue
- ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑÿπŸÖŸäŸÑ Ÿäÿ™ÿßÿ®ÿπ ÿ≥ÿ§ÿßŸÑÿßŸã ÿ≥ÿßÿ®ŸÇÿßŸã ÿ£Ÿà Ÿäÿ∑ŸÑÿ® ÿ™Ÿàÿ∂Ÿäÿ≠ÿßŸã ÿ£ŸÉÿ´ÿ±ÿå ÿßÿπÿ™ÿ®ÿ± ÿßŸÑÿ≥ŸäÿßŸÇ ŸÅŸä ÿßŸÑŸÇÿ±ÿßÿ±

üß† ÿ™ŸÅŸÉŸäÿ± ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑÿ≥ŸäÿßŸÇ:
- ŸáŸÑ Ÿáÿ∞Ÿá ÿßŸÑÿ±ÿ≥ÿßŸÑÿ© ŸÖÿ±ÿ™ÿ®ÿ∑ÿ© ÿ®ÿ¥Ÿäÿ° ÿ™ŸÖ ŸÖŸÜÿßŸÇÿ¥ÿ™Ÿá ŸÖŸÜ ŸÇÿ®ŸÑÿü
- ŸáŸÑ ÿßŸÑÿπŸÖŸäŸÑ ÿ±ÿßÿ∂Ÿç ÿπŸÜ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑÿ≥ÿßÿ®ŸÇÿ© ÿ£ŸÖ Ÿäÿ≠ÿ™ÿßÿ¨ ÿßŸÑŸÖÿ≤ŸäÿØÿü
- ŸáŸÑ ÿßŸÑÿ±ÿØ ÿßŸÑŸÖÿ≠ŸÅŸàÿ∏ ŸÖŸÜÿßÿ≥ÿ® ŸÅŸä Ÿáÿ∞ÿß ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÖÿ≠ÿØÿØÿü
- ŸáŸÑ ÿ™ŸÉÿ±ÿßÿ± ŸÜŸÅÿ≥ ÿßŸÑÿ±ÿØ ÿ≥ŸäŸÅŸäÿØ ÿßŸÑÿπŸÖŸäŸÑ ÿ£ŸÖ ŸÑÿßÿü

ÿßÿÆÿ±ÿ¨ ŸÅŸÇÿ∑ Ÿàÿßÿ≠ÿØÿ© ŸÖŸÜ: reply ÿ£Ÿà skip ÿ£Ÿà continue
"""

        else:
            evaluation_prompt = f"""You are a very strict response quality evaluator for Abar water delivery customer service.

Your task: Determine the appropriate action based on the customer message and the complete conversation history context, not just the current message alone.

Inputs:
- Current customer message: "{user_message}"
- Similar question from database: "{matched_question}"
- Stored response: "{matched_answer}"
{conversation_context}

{history_analysis}

üö® CRITICAL INSTRUCTIONS for using conversation history:
- Carefully review the previous conversation before making any decision
- If the customer asked a question before and we answered it, but they're asking again differently, consider the context
- If the customer is following up on a previous topic, factor that into your decision
- If the agent has already sent a specific response before, don't repeat it unless truly appropriate
- Pay attention to the conversation flow and overall context

Rules (with conversation context awareness):

‚úÖ "reply":
- If the customer message is semantically similar to a known question in the database (even if it contains more than a greeting or thanks), reply using the stored answer.
- OR if the message is **only** a simple genuine greeting or thanks, such as:
    - Greetings: ("Hello", "Hi", "Peace be upon you", "Good morning", "Good evening")
    - Thanks: ("Thanks", "Thank you", "God bless you", "Much appreciated")

- BUT first check conversation context: If this response was sent recently, you might need to choose "continue" instead

üö´ "skip":
- If the message is something like: ("ok", "okay", "fine", "great", "alright", "noted", "sure") ‚Äî it does not require a reply.
- Or if the customer is confirming understanding of something explained in the conversation

üîÅ "continue":
- If the message contains anything beyond a simple greeting or thanks and does not match any known question in the database.
- If the context suggests the customer needs more specialized help
- If the customer is following up on a topic we haven't fully resolved in the previous conversation
- Examples:
    - "Hi, I have a question" ‚Üí continue
    - "Thank you, but I need help" ‚Üí continue
    - "How do I order?" ‚Üí continue
    - "Can I speak to someone?" ‚Üí continue

üìå Enhanced Summary with Context Awareness:
- If there's a semantic match with a known question ‚Üí **reply** (unless context suggests otherwise)
- If it's ONLY a greeting or thanks ‚Üí **reply** (unless it's repetitive in conversation)
- If it's a short acknowledgment ‚Üí **skip**
- If context shows customer needs more help or is following up ‚Üí **continue**
- Everything else ‚Üí **continue**

üß† Contextual Thinking Required:
- Is this message related to something discussed before?
- Is the customer satisfied with the previous answer or needs more?
- Is the stored response appropriate in this specific context?
- Would repeating the same response benefit the customer or not?

Return only one value: reply, skip, or continue
"""

        
        try:
            print(f"ü§ñ ChatGPT evaluation prompt: {evaluation_prompt}")
            
            # Build the complete messages for the API call
            system_content ="""You are an extremely strict evaluator for customer service response quality at Abar Water Delivery.

Your ONLY task: Decide how to handle a customer's message based on its content, whether it matches any known question in the company database, AND most importantly, the complete conversation history context.

üö® CRITICAL: You must analyze the full conversation history before making any decision. The conversation context is just as important as the current message.

Inputs provided:
- Customer message: the message sent by the user.
- Matched question from vector database (if any): the most semantically similar known question.
- Stored answer: the saved response for that matched question (if available).
- Conversation context: previous messages exchanged between customer and agent.

üß† CONTEXTUAL ANALYSIS REQUIRED:
- Review the entire conversation flow before deciding
- Check if we've already answered similar questions
- See if the customer is following up on a previous topic
- Determine if repeating the same response would be helpful or redundant
- Consider the customer's satisfaction level based on their previous responses
- Look for patterns in the conversation that might indicate what the customer really needs

You must classify the message into **only one** of the following:

 reply:
- If the message is **semantically similar** to a known question in the database AND we have a saved answer ‚Äî AND the conversation context supports using this response.
- OR if the message is a **pure standalone greeting or thanks**, with no additional text ‚Äî AND it hasn't been repeatedly used in the conversation.
  - Valid examples: "ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ", "ÿ¥ŸÉÿ±ÿßŸã", "Ÿäÿπÿ∑ŸäŸÉ ÿßŸÑÿπÿßŸÅŸäÿ©", "ŸÖÿ±ÿ≠ÿ®ÿß", "ÿ£ŸáŸÑÿß", "ÿßŸÑŸÑŸá ŸäŸàŸÅŸÇŸÉŸÖ"

 skip:
- If the message contains **acknowledgements** or **neutral confirmations** that don‚Äôt need a response.
  - Examples: "ÿ™ŸÖÿßŸÖ", "ÿ£ŸàŸÉŸä", "ŸÜÿπŸÖ", "ÿ∑Ÿäÿ®", "ÿßŸÜÿ™ŸáŸäÿ™", "ÿ£ŸàŸÉŸäŸá", "ÿÆŸÑÿßÿµ", "ÿ£ŸÉŸäÿØ", "ÿßŸàŸÉŸä ÿ™ŸÖÿßŸÖ"

   continue:
- If the message contains **any other content** (question, request, statement, scheduling info), and we do **not** have a match from the database.
  - Even if the message starts with a greeting or thanks, but continues with more ‚Äî it‚Äôs continue.
  - Examples:
    - "ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖÿå ÿπŸÜÿØŸä ÿßÿ≥ÿ™ŸÅÿ≥ÿßÿ±"
    - "ÿ£ÿ®Ÿä ÿ£ÿ∑ŸÑÿ® ŸÖŸäÿßŸá"
    - "ŸÖÿ™Ÿâ ÿ™ŸàÿµŸÑŸàŸÜÿü"
    - "Ÿäÿπÿ∑ŸäŸÉ ÿßŸÑÿπÿßŸÅŸäÿ©ÿå ÿ®ÿ≥ ÿπŸÜÿØŸä ÿ≥ÿ§ÿßŸÑ"

 Strict enforcement:
- DO NOT reply to partial greetings, mixed messages, or polite phrases that contain extra content ‚Äî unless they match a known question and we have a stored answer.
- DO NOT skip if the message contains any intent or need for help.

Final instruction:
Be extremely conservative AND context-aware. Use `reply` ONLY when:
- The message is a 100% pure greeting/thanks AND it's not repetitive in the conversation, OR
- It has a clear semantic match in the database with a saved answer AND the conversation context supports using this response.

üéØ Key Decision Framework:
1. Is this a pure greeting/thanks? ‚Üí Check if it's repetitive ‚Üí reply or continue
2. Does it match a database question? ‚Üí Check conversation context ‚Üí reply or continue  
3. Is it an acknowledgment? ‚Üí Check if related to previous explanation ‚Üí skip
4. Everything else? ‚Üí continue

Return only one of: `reply`, `skip`, or `continue`.
"""
            
            messages = [
                {"role": "system", "content": system_content},
                {"role": "user", "content": evaluation_prompt}
            ]
            
            # Log the complete prompt before sending to LLM
            if LOGGING_AVAILABLE and journey_id:
                complete_prompt = f"SYSTEM: {system_content}\n\nUSER: {evaluation_prompt}"
                message_journey_logger.add_step(
                    journey_id=journey_id,
                    step_type="embedding_llm_evaluation_prompt",
                    description="Sending evaluation prompt to ChatGPT for embedding response validation",
                    data={
                        "complete_prompt": complete_prompt,
                        "system_prompt": system_content,
                        "user_prompt": evaluation_prompt,
                        "user_message": user_message,
                        "matched_question": matched_question,
                        "matched_answer": matched_answer,
                        "model": "gpt-4o-mini"
                    }
                )
            
            # Time the LLM call
            llm_start_time = time.time()
            
            # Convert to LangChain messages
            langchain_messages = [
                SystemMessage(content=messages[0]["content"]),
                HumanMessage(content=messages[1]["content"])
            ]
            
            # Use LangChain with specific parameters
            temp_llm = self.llm.bind(max_tokens=20, temperature=0.1)
            response = await temp_llm.ainvoke(langchain_messages)
            
            llm_duration = int((time.time() - llm_start_time) * 1000)
            evaluation = response.content.strip().lower()
            
            # Log the complete response from LLM
            if LOGGING_AVAILABLE and journey_id:
                message_journey_logger.log_llm_interaction(
                    journey_id=journey_id,
                    llm_type="openai",
                    prompt=f"SYSTEM: {system_content}\n\nUSER: {evaluation_prompt}",
                    response=evaluation,
                    model="gpt-4o-mini",
                    duration_ms=llm_duration,
                    tokens_used={"total_tokens": None}  # LangChain response doesn't include token usage directly
                )
                
                message_journey_logger.add_step(
                    journey_id=journey_id,
                    step_type="embedding_llm_evaluation_result",
                    description=f"ChatGPT evaluation completed: {evaluation}",
                    data={
                        "evaluation_result": evaluation,
                        "raw_response": response.content,
                        "duration_ms": llm_duration,
                        "tokens_used": None,  # LangChain response doesn't include token usage directly
                        "user_message": user_message,
                        "matched_question": matched_question,
                        "matched_answer": matched_answer
                    }
                )
            
            # Log the evaluation for debugging
            print(f"ü§ñ ChatGPT evaluation result: '{evaluation}'")
            print(f"ü§ñ Raw ChatGPT response: '{response.content}'")
            print(f"ü§ñ User message: '{user_message}'")
            print(f"ü§ñ Matched question: '{matched_question}'")
            print(f"ü§ñ Matched answer: '{matched_answer[:100]}...'")
            
            # Map the response to our action format
            if 'reply' in evaluation:
                print(f"‚úÖ EmbeddingAgent: ChatGPT says REPLY - will send response")
                return {'action': 'reply'}
            elif 'skip' in evaluation:
                print(f"üö´ EmbeddingAgent: ChatGPT says SKIP - no response will be sent")
                return {'action': 'skip'}
            elif 'continue' in evaluation:
                print(f"üîÑ EmbeddingAgent: ChatGPT says CONTINUE - passing to classification agent")
                return {'action': 'continue'}
            else:
                # Default to continue if we can't parse the response
                print(f"‚ö†Ô∏è Could not parse evaluation result '{evaluation}', defaulting to continue")
                return {'action': 'continue'}
                
        except Exception as e:
            print(f"‚ùå EmbeddingAgent: Error evaluating response with ChatGPT: {str(e)}")
            # Default to continue on error
            return {'action': 'continue'}

    async def debug_knowledge_base_structure(self, sample_size: int = 5) -> Dict[str, Any]:
        """
        Debug method to check the knowledge base structure and verify question-answer linking
        """
        print(f"üîç DEBUG: Testing knowledge base structure...")
        
        try:
            # Get a sample of documents from the knowledge base
            from vectorstore.chroma_db import chroma_manager
            
            # Get all documents
            all_data = chroma_manager.get_collection_safe().get(include=["documents", "metadatas", "embeddings"])
            
            if not all_data or not all_data['documents']:
                print(f"‚ùå DEBUG: No documents found in knowledge base")
                return {"status": "error", "message": "No documents found"}
            
            documents = all_data['documents']
            metadatas = all_data['metadatas']
            ids = all_data['ids']
            
            print(f"üìä DEBUG: Found {len(documents)} total documents")
            
            # Count questions and answers
            questions = []
            answers = []
            
            for i, (doc, metadata, doc_id) in enumerate(zip(documents, metadatas, ids)):
                if metadata.get('type') == 'question':
                    questions.append({'doc': doc, 'metadata': metadata, 'id': doc_id})
                else:
                    answers.append({'doc': doc, 'metadata': metadata, 'id': doc_id})
            
            print(f"üìà DEBUG: Found {len(questions)} questions and {len(answers)} answers")
            
            # Test a few question-answer pairs
            test_results = []
            
            for i, question_info in enumerate(questions[:sample_size]):
                print(f"\nüîç DEBUG Test {i+1}:")
                print(f"   Question: {question_info['doc'][:100]}...")
                print(f"   Question ID: {question_info['id']}")
                print(f"   Question Metadata: {question_info['metadata']}")
                
                answer_text = question_info['metadata'].get('answer_text', '')
                if answer_text and answer_text.strip():
                    print(f"   ‚úÖ Found Answer in metadata: {answer_text[:100]}...")
                    
                    test_results.append({
                        "question": question_info['doc'][:100],
                        "answer": answer_text[:100],
                        "status": "success"
                    })
                else:
                    print(f"   ‚ùå No answer found in metadata")
                    test_results.append({
                        "question": question_info['doc'][:100],
                        "answer": None,
                        "status": "no_answer_in_metadata"
                    })
            
            return {
                "status": "success",
                "total_documents": len(documents),
                "questions_count": len(questions),
                "answers_count": len(answers),
                "test_results": test_results
            }
            
        except Exception as e:
            print(f"‚ùå DEBUG: Error testing knowledge base structure: {str(e)}")
            return {"status": "error", "message": str(e)}

# Create and export the instance
embedding_agent = EmbeddingAgent() 