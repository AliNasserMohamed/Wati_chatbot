import os
import openai
import time
from typing import Optional, Dict, Any, Tuple
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from vectorstore.chroma_db import chroma_manager
from utils.language_utils import language_handler

# Import message journey logger for detailed logging
try:
    from utils.message_logger import message_journey_logger
    LOGGING_AVAILABLE = True
except ImportError:
    LOGGING_AVAILABLE = False
    print("âš ï¸ Message journey logger not available - detailed embedding logging disabled")

class EmbeddingAgent:
    def __init__(self):
        # Keep AsyncOpenAI for fallback if needed
        self.openai_client = openai.AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        # Initialize LangChain client for better tracing
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.1,
            api_key=os.getenv("OPENAI_API_KEY"),
            tags=["embedding-agent", "abar-chatbot"]
        )
        self.similarity_threshold = 0.50  # Higher cosine similarity means better match
        
    async def process_message(self, user_message: str, conversation_history: list = None, user_language: str = 'ar', journey_id: str = None) -> Dict[str, Any]:
        """
        Process incoming message by comparing to knowledge base using embeddings
        
        Returns:
        - action: 'reply', 'skip', or 'continue_to_classification'
        - response: the response text if action is 'reply'
        - confidence: confidence score
        - matched_question: the matched question from database
        """
        
        # Detect the actual language of the user message
        detected_user_language = language_handler.detect_language(user_message)
        
        # Search for similar questions in the knowledge base
        search_results = await chroma_manager.search(user_message, n_results=3)
        
        if not search_results:
            return {
                'action': 'continue_to_classification',
                'response': None,
                'confidence': 0,
                'matched_question': None
            }
        
        # Detailed logging: capture all search results for journey logging
        search_results_for_log = []
        
        for i, result in enumerate(search_results, 1):
            similarity_score = result.get('similarity', result.get('distance', 0.0))
            document_preview = result['document'][:100] + "..." if len(result['document']) > 100 else result['document']
            metadata = result.get('metadata', {})
            
            # Capture for detailed logging
            search_results_for_log.append({
                "rank": i,
                "similarity_score": similarity_score,
                "document": result['document'],
                "document_type": metadata.get('type', 'unknown'),
                "has_answer_text": bool(metadata.get('answer_text', '').strip()),
                "has_answer": metadata.get('has_answer'),
                "metadata": metadata
            })
        
        # Log all search results to journey if logging is available
        if LOGGING_AVAILABLE and journey_id:
            message_journey_logger.add_step(
                journey_id=journey_id,
                step_type="embedding_search_results",
                description=f"Found {len(search_results)} similar questions in knowledge base",
                data={
                    "search_results": search_results_for_log,
                    "user_message": user_message,
                    "top_similarity": search_results_for_log[0]["similarity_score"] if search_results_for_log else 0
                }
            )
        
        # Get the best match (highest similarity)
        best_match = search_results[0]
        similarity_score = best_match.get('similarity', 0.0)  # Use 'similarity' not 'cosine_similarity'
        
        # Check if similarity is good enough (higher is better)
        if similarity_score < self.similarity_threshold:
            return {
                'action': 'continue_to_classification',
                'response': None,
                'confidence': 0,
                'matched_question': None
            }
        
        # Get the corresponding answer
        matched_document = best_match['document']
        metadata = best_match['metadata']
        
        # CRITICAL: Always get the answer, never return the question
        final_answer = None
        matched_question_text = None
        
        # If the matched document is a question, get its answer from metadata
        if metadata.get('type') == 'question':
            matched_question_text = matched_document
            answer_text = metadata.get('answer_text', '')  # Get answer from metadata
            
            if answer_text and answer_text.strip():
                final_answer = answer_text.strip()
                
                # Log the matched question-answer pair for journey tracking
                if LOGGING_AVAILABLE and journey_id:
                    message_journey_logger.add_step(
                        journey_id=journey_id,
                        step_type="embedding_qa_match",
                        description="Successfully matched question with answer from knowledge base",
                        data={
                            "matched_question": matched_question_text,
                            "matched_answer": final_answer,
                            "similarity_score": similarity_score,
                            "answer_source": "metadata",
                            "question_metadata": metadata
                        }
                    )
            else:
                final_answer = None
                
        else:
            # The matched document is already an answer (shouldn't happen with new approach)
            final_answer = matched_document
            matched_question_text = "Direct answer match"

        
        # CRITICAL: If no valid answer found, don't reply
        if not final_answer or final_answer.strip() == "":
            return {
                'action': 'skip',
                'response': None,
                'confidence': similarity_score,
                'matched_question': matched_question_text or matched_document,
                'error': 'No valid answer found'
            }
        
        # Check if answer is too short
        if len(final_answer.strip()) < 2:
            return {
                'action': 'skip',
                'response': None,
                'confidence': similarity_score,
                'matched_question': matched_question_text or matched_document,
                'error': 'Answer too short'
            }
        
        # CRITICAL: Check language matching BEFORE processing
        user_language = language_handler.detect_language(user_message)
        answer_language = language_handler.detect_language(final_answer)
        
        # If languages don't match, proceed to classification
        if user_language != answer_language:
            return {
                'action': 'continue_to_classification',
                'response': None,
                'confidence': similarity_score,
                'matched_question': matched_question_text or matched_document,
                'error': f'Language mismatch: user={user_language}, answer={answer_language}'
            }
        
       
        
        # Ask ChatGPT to evaluate if the response is appropriate
        evaluation_result = await self._evaluate_response_with_chatgpt(
            user_message, matched_question_text or matched_document, final_answer, detected_user_language, conversation_history, journey_id
        )
        
        if evaluation_result['action'] == 'reply':
            print(f"ðŸ¤– EmbeddingAgent Reply: {evaluation_result.get('response', final_answer)[:100]}...")
            return {
                'action': 'reply',
                'response': evaluation_result.get('response', final_answer),
                'confidence': similarity_score,
                'matched_question': matched_question_text or matched_document
            }
        elif evaluation_result['action'] == 'skip':
            return {
                'action': 'skip',
                'response': None,
                'confidence': similarity_score,
                'matched_question': matched_question_text or matched_document
            }
        else:
            return {
                'action': 'continue_to_classification',
                'response': None,
                'confidence': similarity_score,
                'matched_question': matched_question_text or matched_document
            }
    
    async def _evaluate_response_with_chatgpt(self, user_message: str, matched_question: str, 
                                            matched_answer: str, language: str, conversation_history: list = None, journey_id: str = None) -> Dict[str, Any]:
        """
        Ask ChatGPT to evaluate if the response is good and appropriate
        """
        
        # First check if the user message and matched answer are in the same language
        user_language = language_handler.detect_language(user_message)
        answer_language = language_handler.detect_language(matched_answer)
        
        # If languages don't match, proceed to classification
        if user_language != answer_language:
            return {'action': 'continue'}
        
        # Format conversation history for context
        conversation_context = ""
        if conversation_history:
            # Get the latest 5 messages for context
            recent_messages = conversation_history[-5:] if len(conversation_history) >= 5 else conversation_history
            
            if language == 'ar':
                conversation_context = "\n\nØ³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© (Ø¢Ø®Ø± 3 Ø±Ø³Ø§Ø¦Ù„):\n"
                for i, msg in enumerate(recent_messages, 1):
                    role = "Ø§Ù„Ø¹Ù…ÙŠÙ„" if msg.get('role') == 'user' else "Ø§Ù„ÙˆÙƒÙŠÙ„"
                    conversation_context += f"{i}. {role}: {msg.get('content', '')}\n"
            else:
                conversation_context = "\n\nConversation context (last 3 messages):\n"
                for i, msg in enumerate(recent_messages, 1):
                    role = "Customer" if msg.get('role') == 'user' else "Agent"
                    conversation_context += f"{i}. {role}: {msg.get('content', '')}\n"
        
        if language == 'ar':
            evaluation_prompt = f"""Ø£Ù†Øª Ù…Ù‚ÙŠÙ… ØµØ§Ø±Ù… Ø¬Ø¯Ø§Ù‹ Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø±Ø¯ÙˆØ¯ ÙÙŠ Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ù„Ø´Ø±ÙƒØ© Ø£Ø¨Ø§Ø± Ù„ØªÙˆØµÙŠÙ„ Ø§Ù„Ù…ÙŠØ§Ù‡.

Ù…Ù‡Ù…ØªÙƒ Ø§Ù„ÙˆØ­ÙŠØ¯Ø©: ØªØµÙ†ÙŠÙ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¨Ø¯Ù‚Ø© Ø¥Ù„Ù‰ ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø«Ù„Ø§Ø« Ø­Ø§Ù„Ø§Øª Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©.

- Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠØ©: "{user_message}"
- Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª : "{matched_question}"
- Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ø­ÙÙˆØ¸: "{matched_answer}"

-Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:
{conversation_context}

âš ï¸ **Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:**
Ù‚Ø¨Ù„ Ø§Ø®ØªÙŠØ§Ø± "reply"ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªØªØ£ÙƒØ¯ Ø£Ù† Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙˆØ§Ù„Ø³Ø¤Ø§Ù„ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù‡Ù…Ø§ **Ù†ÙØ³ Ø§Ù„Ù…Ø¹Ù†Ù‰ ÙˆØ§Ù„Ù‚ØµØ¯** ØªÙ…Ø§Ù…Ø§Ù‹.
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø¹Ù†Ù‰ Ù…Ø®ØªÙ„Ù Ø£Ùˆ Ø§Ù„Ù‚ØµØ¯ Ù…Ø®ØªÙ„ÙØŒ Ø§Ø®ØªØ± "continue" Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù…ØªØ´Ø§Ø¨Ù‡Ø©
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙŠØ³Ø£Ù„ Ø¹Ù† Ø´ÙŠØ¡ ÙˆØ§Ù„Ø³Ø¤Ø§Ù„ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù† Ø´ÙŠØ¡ Ø¢Ø®Ø±ØŒ Ø§Ø®ØªØ± "continue"
- ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø¹Ù†Ù‰ ÙˆØ§Ù„Ù‚ØµØ¯ Ù…ØªØ·Ø§Ø¨Ù‚ ØªÙ…Ø§Ù…Ø§Ù‹ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø®ØªÙŠØ§Ø± "reply"

Ø§Ù„ØªØµÙ†ÙŠÙ ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:

ðŸŸ¢ "reply":
- âœ… ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙˆØ§Ù„Ø³Ø¤Ø§Ù„ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù‡Ù…Ø§ **Ù†ÙØ³ Ø§Ù„Ù…Ø¹Ù†Ù‰ ÙˆØ§Ù„Ù‚ØµØ¯ ØªÙ…Ø§Ù…Ø§Ù‹** ÙˆÙƒØ§Ù† Ù„Ø¯ÙŠÙ†Ø§ Ø±Ø¯ Ù…Ø­ÙÙˆØ¸ Ù…Ù†Ø§Ø³Ø¨
- âœ… Ø£Ùˆ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ù…Ø¬Ø±Ø¯ ØªØ­ÙŠØ© Ø£Ùˆ Ø´ÙƒØ± Ø¨Ø³ÙŠØ· Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù…Ø­ØªÙˆÙ‰ Ø¥Ø¶Ø§ÙÙŠ (Ù…Ø¹ Ø£Ùˆ Ø¨Ø¯ÙˆÙ† Ø¹Ù„Ø§Ù…Ø§Øª ØªØ±Ù‚ÙŠÙ… Ù…Ø«Ù„ Ø§Ù„Ù†Ù‚Ø§Ø· Ø£Ùˆ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª)
  - Ù…Ø«Ù„: (Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…ØŒ Ù…Ø±Ø­Ø¨Ø§ØŒ Ø£Ù‡Ù„Ø§Ù‹ØŒ Ù‡Ù„Ø§ØŒ Ù‡Ù„Ø§ ÙˆØºÙ„Ø§ØŒ ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±ØŒ Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±ØŒ Ø´ÙƒØ±Ø§Ù‹ØŒ ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ©ØŒ Ø¬Ø²Ø§Ùƒ Ø§Ù„Ù„Ù‡ Ø®ÙŠØ±ØŒ Ø§Ù„Ù„Ù‡ ÙŠÙˆÙÙ‚ÙƒÙ…ØŒ Ø´ÙƒØ±Ø§ Ù„ÙƒØŒ Ù…Ø´ÙƒÙˆØ±)
  - Ø£Ùˆ Ù†ÙØ³ Ø§Ù„ØªØ­ÙŠØ§Øª Ù…Ø¹ Ø¹Ù„Ø§Ù…Ø§Øª ØªØ±Ù‚ÙŠÙ… Ù…Ø«Ù„: (Ù‡Ù„Ø§...ØŒ Ù…Ø±Ø­Ø¨Ø§.ØŒ Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…!!ØŒ Ø´ÙƒØ±Ø§...) ÙŠØ¬Ø¨ Ø§Ø®ØªÙŠØ§Ø± reply

ðŸŸ¡ "skip":
- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ù„Ø§ ØªØªØ·Ù„Ø¨ Ø±Ø¯ Ù…Ø«Ù„: (ØªÙ…Ø§Ù…ØŒ Ø·ÙŠØ¨ØŒ Ø£ÙˆÙƒØŒ Ø£ÙˆÙƒÙŠØŒ ØªÙ…Ø§Ù… Ø§Ù„ØªÙ…Ø§Ù…ØŒ Ø®Ù„Ø§Øµ)

ðŸ”´ "continue":
- Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† ØªØ­ÙŠØ© Ø£Ùˆ Ø´ÙƒØ± Ø¨Ø³ÙŠØ·
- ÙˆÙ„Ù… Ù†Ø¬Ø¯ Ù„Ù‡Ø§ ØªØ·Ø§Ø¨Ù‚Ù‹Ø§ ÙˆØ§Ø¶Ø­Ù‹Ø§ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø£ÙŠ Ù„Ù… ØªÙƒÙ† Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„Ø³Ø¤Ø§Ù„ Ù…ÙˆØ¬ÙˆØ¯ Ù„Ø¯ÙŠÙ†Ø§)
- Ø£Ùˆ ÙƒØ§Ù†Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªØ­ÙŠØ© Ø£Ùˆ Ø´ÙƒØ± Ù„ÙƒÙ† Ù…Ø±ÙÙ‚Ø© Ø¨Ø³Ø¤Ø§Ù„ Ø£Ùˆ Ø·Ù„Ø¨
- Ø§Ø°Ø§ ÙƒØ§Ù† Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙŠØ´ÙŠØ± Ø§Ù… Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙŠØ³ØªÙØ³Ø± Ø¹Ù† Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ùˆ Ø§Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ¬Ø§Ø±ÙŠØ© Ø§Ùˆ Ø§Ù„Ù…Ø¯Ù† Ø§Ùˆ Ø§Ù„Ø§Ø³Ø¹Ø§Ø±
- ðŸš¨ Ø¥Ø°Ø§ Ø°ÙƒØ± Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¹Ù„Ø§Ù…Ø§Øª ØªØ¬Ø§Ø±ÙŠØ© Ù„Ù„Ù…ÙŠØ§Ù‡ - Ù‡Ø°Ù‡ Ø¹Ù„Ø§Ù…Ø§Øª Ù…ÙŠØ§Ù‡ Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙˆÙŠØ¬Ø¨ Ø¥Ø±Ø³Ø§Ù„Ù‡Ø§ Ù„Ù„ØªØµÙ†ÙŠÙ
- Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ¬Ø§Ø±ÙŠØ© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©: Ù†Ø³ØªÙ„Ù‡ØŒ Ø£ÙƒÙˆØ§ÙÙŠÙ†Ø§ØŒ Ø§Ù„Ø¹ÙŠÙ†ØŒ Ø§Ù„Ù‚ØµÙŠÙ…ØŒ Ø§Ù„Ù…Ø±Ø§Ø¹ÙŠØŒ Ù†ÙˆÙØ§ØŒ Ù†Ù‚ÙŠØŒ ØªØ§Ù†ÙŠØ§ØŒ ØµØ§ÙÙŠØ©ØŒ Ø¨Ù†Ù…Ø§ØŒ Ø£Ø±ÙˆÙ‰ØŒ Ù…Ø³Ø§Ø¡ØŒ Ø³Ø¯ÙŠØ±ØŒ ØµØ­ØªÙƒØŒ ØµØ­ØªÙŠÙ†ØŒ ÙˆÙŠØŒ Ø§Ù„Ù…Ù†Ù‡Ù„ØŒ Ø­Ù„ÙˆØ©ØŒ Ù‡Ù†Ø§ØŒ ØµÙØ§ Ù…ÙƒØ©ØŒ Ø£ÙˆØ³ÙƒØ§
- ðŸ” Ù…Ù‡Ù…: ÙŠÙ…ÙƒÙ† Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ¬Ø§Ø±ÙŠØ© Ù…Ù† Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ø£ÙŠØ¶Ø§Ù‹ - Ø¥Ø°Ø§ Ø°ÙÙƒØ±Øª Ø¹Ù„Ø§Ù…Ø© ØªØ¬Ø§Ø±ÙŠØ© ÙÙŠ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©ØŒ ÙŠØ¬Ø¨ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ù„Ù„ØªØµÙ†ÙŠÙ 

â—ï¸Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ø±Ø§Ø¹Ø§Ø© Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:
- **Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©**: Ø¥Ø°Ø§ Ø£Ø±Ø³Ù„Ù†Ø§ Ù†ÙØ³ Ø§Ù„Ù†ÙˆØ¹ Ù…Ù† Ø§Ù„Ø±Ø¯ (Ù…Ø«Ù„ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø£Ùˆ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø¹Ø§Ø±) Ø®Ù„Ø§Ù„ Ø¢Ø®Ø± 3-5 Ø±Ø³Ø§Ø¦Ù„ØŒ Ø§Ø®ØªØ± "continue"
- **Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ØªØ·ÙˆÙŠØ±ÙŠ**: Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙŠØ³Ø£Ù„ Ø¹Ù† Ø´ÙŠØ¡ Ù…Ø­Ø¯Ø¯ Ø¨Ø¹Ø¯ Ø£Ù† Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø±Ø¯ Ø¹Ø§Ù…ØŒ Ø§Ø®ØªØ± "continue" Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹

â—ï¸Ù…Ù„Ø­ÙˆØ¸Ø©:
- Ø¥Ø°Ø§ ÙˆÙØ¬Ø¯ ØªØ·Ø§Ø¨Ù‚ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø±Ø¯ Ù…Ø­ÙÙˆØ¸ ÙˆÙ„Ù… Ù†Ø±Ø³Ù„Ù‡ Ù…Ø¤Ø®Ø±Ø§Ù‹ØŒ Ø§Ø®ØªØ± "reply"
- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø±Ø³Ø§Ù„Ø© ÙÙ‚Ø· "Ø´ÙƒØ±Ø§Ù‹" Ø£Ùˆ "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…"ØŒ Ø§Ø®ØªØ± "reply"
- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ø«Ù„ "ØªÙ…Ø§Ù…" Ø£Ùˆ "Ø£ÙˆÙƒ"ØŒ Ø§Ø®ØªØ± "skip"
- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø£Ùˆ Ø§Ø³ØªÙØ³Ø§Ø± ÙˆÙ„Ù… Ù†Ø¬Ø¯ Ù„Ù‡Ø§ ØªØ·Ø§Ø¨Ù‚Ø§Ù‹ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ø§Ø®ØªØ± "continue"
 â—ï¸Ù…Ù„Ø­ÙˆØ¸Ù‡ Ù…Ù‡Ù…Ø© Ø¬Ø¯Ø§ Ø¬Ø¯Ø§ 
- Ø§Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ø§Ù„ Ø¹Ù† Ù…Ø§Ø±ÙƒØ© Ù…ÙŠØ§Ù‡ Ù…Ø¹ÙŠÙ†Ø© Ø§Ùˆ Ù…Ù†ØªØ¬ Ù…Ø¹ÙŠÙ† Ø§Ùˆ Ø³Ø¹Ø± Ù…Ù†ØªØ¬ Ù…Ø¹ÙŠÙ† Ø§Ùˆ Ø§Ù„Ø³ÙˆØ§Ù„ Ø¹Ù† Ø§Ù„Ø§Ø³Ø¹Ø§Ø± Ø§Ùˆ Ø§Ù„Ù…Ø§Ø±ÙƒØ§Øª ÙÙŠ Ù…Ø¯ÙŠÙ†Ø© Ù…Ø¹ÙŠÙ†Ø©Ø§Ø®ØªØ± "continue"
- **ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©**: Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ø­ÙÙˆØ¸ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø· Ø£Ùˆ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø±Ø³Ù„Ù†Ø§Ù‡Ø§ Ù„Ù„Ø¹Ù…ÙŠÙ„ ÙÙŠ Ø¢Ø®Ø± 3 Ø±Ø³Ø§Ø¦Ù„ØŒ Ø§Ø®ØªØ± "continue"

Ø§Ø®Ø±Ø¬ ÙÙ‚Ø· ÙˆØ§Ø­Ø¯Ø© Ù…Ù†: reply Ø£Ùˆ skip Ø£Ùˆ continue
"""

        else:
            evaluation_prompt = f"""You are a very strict response quality evaluator for Abar water delivery customer service.

Your task: Determine the appropriate action based on the customer message, database match, and conversation history context.

Inputs:
- Current customer message: "{user_message}"
- Similar question from database: "{matched_question}"
- Stored response: "{matched_answer}"
{conversation_context}

âš ï¸ **Required Semantic Meaning Check:**
Before choosing "reply", you must ensure that the customer message and the database question have **exactly the same meaning and intent**.
- If the meaning is different or the intent is different, choose "continue" even if the words are similar
- If the customer is asking about one thing and the database question is about something else, choose "continue"
- Only if the meaning and intent are exactly the same, you may choose "reply"

Rules with conversation context consideration:

âœ… "reply":
- ONLY if the customer message and database question have **exactly the same meaning and intent** and we have an appropriate stored response
- OR if the message is **only** a simple genuine greeting or thanks, such as:
    - Greetings: ("Hello", "Hi", "Peace be upon you", "Good morning", "Good evening")
    - Thanks: ("Thanks", "Thank you", "God bless you", "Much appreciated")

ðŸš« "skip":
- If the message is something like: ("ok", "okay", "fine", "great", "alright", "noted", "sure") â€” it does not require a reply.

ðŸ” "continue":
- If the message contains anything beyond a simple greeting or thanks and does not match any known question in the database.
- ðŸš¨ If customer mentions water brand names - these are real water brands and should be sent to classification
- Common water brand names: Ù†Ø³ØªÙ„Ù‡ (Nestle), Ø£ÙƒÙˆØ§ÙÙŠÙ†Ø§ (Aquafina), Ø§Ù„Ø¹ÙŠÙ† (Al-Ain), Ø§Ù„Ù‚ØµÙŠÙ… (Al-Qassim), Ø§Ù„Ù…Ø±Ø§Ø¹ÙŠ (Almarai), Ù†ÙˆÙØ§ (Nova), Ù†Ù‚ÙŠ (Naqi), ØªØ§Ù†ÙŠØ§ (Tania), ØµØ§ÙÙŠØ© (Safia), Ø¨Ù†Ù…Ø§ (Banama), Ø£Ø±ÙˆÙ‰ (Arwa), Ù…Ø³Ø§Ø¡ (Massa), Ø³Ø¯ÙŠØ± (Sudair), ØµØ­ØªÙƒ (Sahtak), ØµØ­ØªÙŠÙ† (Sahtain), ÙˆÙŠ (Wi), Ø§Ù„Ù…Ù†Ù‡Ù„ (Al-Manhal), Ø­Ù„ÙˆØ© (Helwa), Ù‡Ù†Ø§ (Hena), ØµÙØ§ Ù…ÙƒØ© (Safa Makkah), Ø£ÙˆØ³ÙƒØ§ (Oska)
- ðŸ” Important: Brand names can also be identified from conversation history context - if a brand was mentioned in previous messages, current message should be sent to classification
- Examples:
    - "Hi, I have a question" â†’ continue
    - "Thank you, but I need help" â†’ continue
    - "How do I order?" â†’ continue
    - "Can I speak to someone?" â†’ continue

ðŸ”„ **Conversation History Rules**:
- **Repeated Response Types**: If we sent the same type of response (like app links or pricing info) within the last 3-5 messages, choose "continue"
- **Contextual Follow-up**: If the customer asks for something specific after receiving a general response, choose "continue" for more detailed information
- **Link/Information Analysis**: If the stored response contains links or information we already sent to the customer in the last 3 messages, choose "continue"

ðŸ“Œ Summary:
- If there's a semantic match with a known question AND we haven't sent similar response recently â†’ **reply**
- If it's ONLY a greeting or thanks â†’ **reply**
- If it's a short acknowledgment â†’ **skip**
- If we recently sent similar response or customer needs follow-up â†’ **continue**
- Everything else â†’ **continue**

Return only one value: reply, skip, or continue
"""

        
        try:
            # Build the complete messages for the API call
            system_content ="""You are an extremely strict evaluator for customer service response quality at Abar Water Delivery.

Your ONLY task: Decide how to handle a customer's message based on its content and whether it has the **exact same meaning** as any known question in the company database.

âš ï¸ **CRITICAL SEMANTIC MEANING REQUIREMENT:**
You must perform a strict semantic meaning check. The customer message and database question must have **exactly the same meaning and intent** - not just similar words or topics.

Inputs provided:
- Customer message: the message sent by the user.
- Matched question from vector database (if any): the most semantically similar known question.
- Stored answer: the saved response for that matched question (if available).
- Conversation context: last few messages exchanged.

You must classify the message into **only one** of the following:

 reply:
- ONLY if the message has **exactly the same meaning and intent** as a known question in the database AND we have a saved answer
- OR if the message is a **pure standalone greeting or thanks**, with no additional text.
  - Valid examples: "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…", "Ø´ÙƒØ±Ø§Ù‹", "ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ©", "Ù…Ø±Ø­Ø¨Ø§", "Ø£Ù‡Ù„Ø§", "Ø§Ù„Ù„Ù‡ ÙŠÙˆÙÙ‚ÙƒÙ…"

 skip:
- If the message contains **acknowledgements** or **neutral confirmations** that donâ€™t need a response.
  - Examples: "ØªÙ…Ø§Ù…", "Ø£ÙˆÙƒÙŠ", "Ù†Ø¹Ù…", "Ø·ÙŠØ¨", "Ø§Ù†ØªÙ‡ÙŠØª", "Ø£ÙˆÙƒÙŠÙ‡", "Ø®Ù„Ø§Øµ", "Ø£ÙƒÙŠØ¯", "Ø§ÙˆÙƒÙŠ ØªÙ…Ø§Ù…"

   continue:
- If the message contains **any other content** (question, request, statement, scheduling info), and we do **not** have a match from the database.
  - Even if the message starts with a greeting or thanks, but continues with more â€” it's continue.
  - ðŸš¨ If customer mentions water brand names - these are real water brands and should be sent to classification
  - Common water brand names: Ù†Ø³ØªÙ„Ù‡ (Nestle), Ø£ÙƒÙˆØ§ÙÙŠÙ†Ø§ (Aquafina), Ø§Ù„Ø¹ÙŠÙ† (Al-Ain), Ø§Ù„Ù‚ØµÙŠÙ… (Al-Qassim), Ø§Ù„Ù…Ø±Ø§Ø¹ÙŠ (Almarai), Ù†ÙˆÙØ§ (Nova), Ù†Ù‚ÙŠ (Naqi), ØªØ§Ù†ÙŠØ§ (Tania), ØµØ§ÙÙŠØ© (Safia), Ø¨Ù†Ù…Ø§ (Banama), Ø£Ø±ÙˆÙ‰ (Arwa), Ù…Ø³Ø§Ø¡ (Massa), Ø³Ø¯ÙŠØ± (Sudair), ØµØ­ØªÙƒ (Sahtak), ØµØ­ØªÙŠÙ† (Sahtain), ÙˆÙŠ (Wi), Ø§Ù„Ù…Ù†Ù‡Ù„ (Al-Manhal), Ø­Ù„ÙˆØ© (Helwa), Ù‡Ù†Ø§ (Hena), ØµÙØ§ Ù…ÙƒØ© (Safa Makkah), Ø£ÙˆØ³ÙƒØ§ (Oska)
  - ðŸ” Important: Brand names can also be identified from conversation history context - if a brand was mentioned in previous messages, current message should be sent to classification
  - Examples:
    - "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…ØŒ Ø¹Ù†Ø¯ÙŠ Ø§Ø³ØªÙØ³Ø§Ø±"
    - "Ø£Ø¨ÙŠ Ø£Ø·Ù„Ø¨ Ù…ÙŠØ§Ù‡"
    - "Ù…ØªÙ‰ ØªÙˆØµÙ„ÙˆÙ†ØŸ"
    - "ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ©ØŒ Ø¨Ø³ Ø¹Ù†Ø¯ÙŠ Ø³Ø¤Ø§Ù„"

 Strict enforcement:
- DO NOT reply to partial greetings, mixed messages, or polite phrases that contain extra content â€” unless they match a known question and we have a stored answer.
- DO NOT skip if the message contains any intent or need for help.

Final instruction:
Be extremely conservative. Use `reply` ONLY when:
- The message is a 100% pure greeting/thanks, OR
- It has **exactly the same meaning and intent** as a question in the database with a saved answer.

âš ï¸ Remember: Similar words â‰  Same meaning. The customer's intent must be identical to the database question's intent.

Return only one of: `reply`, `skip`, or `continue`.
"""
            
            messages = [
                {"role": "system", "content": system_content},
                {"role": "user", "content": evaluation_prompt}
            ]
            
            # Log the complete prompt before sending to LLM
            if LOGGING_AVAILABLE and journey_id:
                complete_prompt = f"SYSTEM: {system_content}\n\nUSER: {evaluation_prompt}"
                message_journey_logger.add_step(
                    journey_id=journey_id,
                    step_type="embedding_llm_evaluation_prompt",
                    description="Sending evaluation prompt to ChatGPT for embedding response validation",
                    data={
                        "complete_prompt": complete_prompt,
                        "system_prompt": system_content,
                        "user_prompt": evaluation_prompt,
                        "user_message": user_message,
                        "matched_question": matched_question,
                        "matched_answer": matched_answer,
                        "model": "gpt-4o-mini"
                    }
                )
            
            # Time the LLM call
            llm_start_time = time.time()
            
            # Convert to LangChain messages
            langchain_messages = [
                SystemMessage(content=messages[0]["content"]),
                HumanMessage(content=messages[1]["content"])
            ]
            
            # Use LangChain with specific parameters
            temp_llm = self.llm.bind(max_tokens=20, temperature=0.1)
            response = await temp_llm.ainvoke(langchain_messages)
            
            llm_duration = int((time.time() - llm_start_time) * 1000)
            evaluation = response.content.strip().lower()
            
            # Log the complete response from LLM
            if LOGGING_AVAILABLE and journey_id:
                message_journey_logger.log_llm_interaction(
                    journey_id=journey_id,
                    llm_type="openai",
                    prompt=f"SYSTEM: {system_content}\n\nUSER: {evaluation_prompt}",
                    response=evaluation,
                    model="gpt-4o-mini",
                    duration_ms=llm_duration,
                    tokens_used={"total_tokens": None}  # LangChain response doesn't include token usage directly
                )
                
                message_journey_logger.add_step(
                    journey_id=journey_id,
                    step_type="embedding_llm_evaluation_result",
                    description=f"ChatGPT evaluation completed: {evaluation}",
                    data={
                        "evaluation_result": evaluation,
                        "raw_response": response.content,
                        "duration_ms": llm_duration,
                        "tokens_used": None,  # LangChain response doesn't include token usage directly
                        "user_message": user_message,
                        "matched_question": matched_question,
                        "matched_answer": matched_answer
                    }
                )
            
            # Map the response to our action format
            if 'reply' in evaluation:
                return {'action': 'reply'}
            elif 'skip' in evaluation:
                return {'action': 'skip'}
            elif 'continue' in evaluation:
                return {'action': 'continue'}
            else:
                # Default to continue if we can't parse the response
                return {'action': 'continue'}
                
        except Exception as e:
            # Default to continue on error
            return {'action': 'continue'}

    async def debug_knowledge_base_structure(self, sample_size: int = 5) -> Dict[str, Any]:
        """
        Debug method to check the knowledge base structure and verify question-answer linking
        """

        
        try:
            # Get a sample of documents from the knowledge base
            from vectorstore.chroma_db import chroma_manager
            
            # Get all documents
            all_data = chroma_manager.get_collection_safe().get(include=["documents", "metadatas", "embeddings"])
            
            if not all_data or not all_data['documents']:
                return {"status": "error", "message": "No documents found"}
            
            documents = all_data['documents']
            metadatas = all_data['metadatas']
            ids = all_data['ids']
            

            
            # Count questions and answers
            questions = []
            answers = []
            
            for i, (doc, metadata, doc_id) in enumerate(zip(documents, metadatas, ids)):
                if metadata.get('type') == 'question':
                    questions.append({'doc': doc, 'metadata': metadata, 'id': doc_id})
                else:
                    answers.append({'doc': doc, 'metadata': metadata, 'id': doc_id})
            

            
            # Test a few question-answer pairs
            test_results = []
            
            for i, question_info in enumerate(questions[:sample_size]):
                answer_text = question_info['metadata'].get('answer_text', '')
                if answer_text and answer_text.strip():
                    test_results.append({
                        "question": question_info['doc'][:100],
                        "answer": answer_text[:100],
                        "status": "success"
                    })
                else:
                    test_results.append({
                        "question": question_info['doc'][:100],
                        "answer": None,
                        "status": "no_answer_in_metadata"
                    })
            
            return {
                "status": "success",
                "total_documents": len(documents),
                "questions_count": len(questions),
                "answers_count": len(answers),
                "test_results": test_results
            }
            
        except Exception as e:
            return {"status": "error", "message": str(e)}

# Create and export the instance
embedding_agent = EmbeddingAgent() 