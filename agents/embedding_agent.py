import os
import openai
import time
from typing import Optional, Dict, Any, Tuple
from vectorstore.chroma_db import chroma_manager
from utils.language_utils import language_handler

# Import message journey logger for detailed logging
try:
    from utils.message_logger import message_journey_logger
    LOGGING_AVAILABLE = True
except ImportError:
    LOGGING_AVAILABLE = False
    print("‚ö†Ô∏è Message journey logger not available - detailed embedding logging disabled")

class EmbeddingAgent:
    def __init__(self):
        self.openai_client = openai.AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.similarity_threshold = 0.50  # Higher cosine similarity means better match
        self.high_similarity_threshold = 0.60  # Very high similarity threshold for direct answers
        
    async def process_message(self, user_message: str, conversation_history: list = None, user_language: str = 'ar', journey_id: str = None) -> Dict[str, Any]:
        """
        Process incoming message by comparing to knowledge base using embeddings
        
        Returns:
        - action: 'reply', 'skip', or 'continue_to_classification'
        - response: the response text if action is 'reply'
        - confidence: confidence score
        - matched_question: the matched question from database
        """
        
        print(f"üîç EmbeddingAgent: Processing message: '{user_message[:50]}...'")
        
        # Detect the actual language of the user message
        detected_user_language = language_handler.detect_language(user_message)
        print(f"üåê EmbeddingAgent: User message language detected as: {detected_user_language}")
        
        # Search for similar questions in the knowledge base
        search_results = await chroma_manager.search(user_message, n_results=3)
        
        if not search_results:
            print(f"üì≠ EmbeddingAgent: No knowledge base matches found")
            return {
                'action': 'continue_to_classification',
                'response': None,
                'confidence': 0,
                'matched_question': None
            }
        
        # Print detailed information about all search results
        print(f"üìä EmbeddingAgent: Found {len(search_results)} similar results:")
        
        # Detailed logging: capture all search results for journey logging
        search_results_for_log = []
        
        for i, result in enumerate(search_results, 1):
            similarity_score = result.get('similarity', result.get('distance', 0.0))
            document_preview = result['document'][:100] + "..." if len(result['document']) > 100 else result['document']
            metadata = result.get('metadata', {})
            
            print(f"   Result {i}:")
            print(f"   - Similarity Score: {similarity_score:.4f}")
            print(f"   - Document Type: {metadata.get('type', 'unknown')}")
            print(f"   - Content Preview: {document_preview}")
            print(f"   - Has Answer Text: {bool(metadata.get('answer_text', '').strip())}")
            print(f"   - Full Metadata: {metadata}")
            print(f"   ---")
            
            # Additional debugging: Check if this is a question without an answer
            if metadata.get('type') == 'question':
                answer_text = metadata.get('answer_text', '')
                if not answer_text or not answer_text.strip():
                    print(f"   ‚ö†Ô∏è  WARNING: Question without answer text!")
                elif metadata.get('has_answer') == False:
                    print(f"   ‚ÑπÔ∏è  Info: Question marked as having no answer")
            
            # Capture for detailed logging
            search_results_for_log.append({
                "rank": i,
                "similarity_score": similarity_score,
                "document": result['document'],
                "document_type": metadata.get('type', 'unknown'),
                "has_answer_text": bool(metadata.get('answer_text', '').strip()),
                "has_answer": metadata.get('has_answer'),
                "metadata": metadata
            })
        
        # Log all search results to journey if logging is available
        if LOGGING_AVAILABLE and journey_id:
            message_journey_logger.add_step(
                journey_id=journey_id,
                step_type="embedding_search_results",
                description=f"Found {len(search_results)} similar questions in knowledge base",
                data={
                    "search_results": search_results_for_log,
                    "user_message": user_message,
                    "top_similarity": search_results_for_log[0]["similarity_score"] if search_results_for_log else 0
                }
            )
        
        # Get the best match (highest similarity)
        best_match = search_results[0]
        similarity_score = best_match.get('similarity', 0.0)  # Use 'similarity' not 'cosine_similarity'
        
        print(f"üéØ EmbeddingAgent: Best match selected:")
        print(f"   - Question: {best_match['document'][:100]}...")
        print(f"   - Similarity: {similarity_score:.4f}")
        print(f"   - Type: {best_match.get('metadata', {}).get('type', 'unknown')}")
        print(f"   - Metadata: {best_match['metadata']}")
        print(f"   - Has Answer Text: {bool(best_match.get('metadata', {}).get('answer_text', '').strip())}")
        
        # Check if similarity is good enough (higher is better)
        if similarity_score < self.similarity_threshold:
            print(f"‚ùå EmbeddingAgent: Similarity too low ({similarity_score:.4f} < {self.similarity_threshold})")
            return {
                'action': 'continue_to_classification',
                'response': None,
                'confidence': 0,
                'matched_question': None
            }
        
        # Get the corresponding answer
        matched_document = best_match['document']
        metadata = best_match['metadata']
        
        print(f"üîç EmbeddingAgent: Processing matched result...")
        print(f"   - Matched Document: {matched_document[:100]}...")
        print(f"   - Document Type: {metadata.get('type', 'unknown')}")
        print(f"   - Metadata: {metadata}")
        
        # CRITICAL: Always get the answer, never return the question
        final_answer = None
        matched_question_text = None
        
        # If the matched document is a question, get its answer from metadata
        if metadata.get('type') == 'question':
            matched_question_text = matched_document
            answer_text = metadata.get('answer_text', '')  # Get answer from metadata
            
            print(f"   - This is a QUESTION: '{matched_question_text[:50]}...'")
            print(f"   - Answer from metadata: '{answer_text[:100] if answer_text else 'No answer'}...'")
            
            if answer_text and answer_text.strip():
                final_answer = answer_text.strip()
                print(f"   - ‚úÖ Found ANSWER in metadata: '{final_answer[:100]}...'")
                
                # Log the matched question-answer pair for journey tracking
                if LOGGING_AVAILABLE and journey_id:
                    message_journey_logger.add_step(
                        journey_id=journey_id,
                        step_type="embedding_qa_match",
                        description="Successfully matched question with answer from knowledge base",
                        data={
                            "matched_question": matched_question_text,
                            "matched_answer": final_answer,
                            "similarity_score": similarity_score,
                            "answer_source": "metadata",
                            "question_metadata": metadata
                        }
                    )
            else:
                print(f"   - ‚ùå No answer found in question metadata")
                final_answer = None
                
        else:
            # The matched document is already an answer (shouldn't happen with new approach)
            print(f"   - ‚ö†Ô∏è  Matched document appears to be an answer directly (unexpected with new approach)")
            final_answer = matched_document
            matched_question_text = "Direct answer match"
        
        # CRITICAL VALIDATION: Never return a question as an answer
        if final_answer and matched_question_text:
            # Check if the answer is the same as the question (data corruption check)
            if final_answer.strip() == matched_question_text.strip():
                print(f"üö´ EmbeddingAgent: Answer is identical to question - data corruption detected!")
                print(f"   - Question: {matched_question_text}")
                print(f"   - Answer: {final_answer}")
                return {
                    'action': 'skip',
                    'response': None,
                    'confidence': similarity_score,
                    'matched_question': matched_question_text,
                    'error': 'Answer identical to question'
                }
        
        # CRITICAL: If no valid answer found, don't reply
        if not final_answer or final_answer.strip() == "":
            print(f"üö´ EmbeddingAgent: No valid answer found - skipping reply")
            print(f"   - Question: {matched_question_text}")
            print(f"   - Answer: '{final_answer}'")
            return {
                'action': 'skip',
                'response': None,
                'confidence': similarity_score,
                'matched_question': matched_question_text or matched_document,
                'error': 'No valid answer found'
            }
        
        # Check if answer is too short
        if len(final_answer.strip()) < 3:
            print(f"üö´ EmbeddingAgent: Answer too short - skipping reply")
            print(f"   - Answer: '{final_answer}'")
            return {
                'action': 'skip',
                'response': None,
                'confidence': similarity_score,
                'matched_question': matched_question_text or matched_document,
                'error': 'Answer too short'
            }
        
        print(f"üìù EmbeddingAgent: Valid answer found!")
        print(f"   - Question: {matched_question_text[:50]}...")
        print(f"   - Answer: {final_answer[:100]}...")
        print(f"   - Answer Length: {len(final_answer)} characters")
        
        # CRITICAL: Check language matching BEFORE processing
        user_language = language_handler.detect_language(user_message)
        answer_language = language_handler.detect_language(final_answer)
        
        print(f"üåê Language Check:")
        print(f"   - User message language: {user_language}")
        print(f"   - Answer language: {answer_language}")
        
        # If languages don't match, skip the response
        if user_language != answer_language:
            print(f"üö´ Language mismatch: user={user_language}, answer={answer_language} - skipping response")
            return {
                'action': 'skip',
                'response': None,
                'confidence': similarity_score,
                'matched_question': matched_question_text or matched_document,
                'error': f'Language mismatch: user={user_language}, answer={answer_language}'
            }
        
        # For very high similarity, use answer directly
        if similarity_score >= self.high_similarity_threshold:
            print(f"‚úÖ EmbeddingAgent: Very high similarity ({similarity_score:.4f}) - using answer directly")
            return {
                'action': 'reply',
                'response': final_answer,
                'confidence': similarity_score,
                'matched_question': matched_question_text or matched_document
            }
        
        # Ask ChatGPT to evaluate if the response is appropriate
        evaluation_result = await self._evaluate_response_with_chatgpt(
            user_message, matched_question_text or matched_document, final_answer, detected_user_language, conversation_history, journey_id
        )
        
        print(f"ü§ñ EmbeddingAgent: ChatGPT evaluation: {evaluation_result}")
        
        if evaluation_result['action'] == 'reply':
            return {
                'action': 'reply',
                'response': evaluation_result.get('response', final_answer),
                'confidence': similarity_score,
                'matched_question': matched_question_text or matched_document
            }
        elif evaluation_result['action'] == 'skip':
            return {
                'action': 'skip',
                'response': None,
                'confidence': similarity_score,
                'matched_question': matched_question_text or matched_document
            }
        else:
            return {
                'action': 'continue_to_classification',
                'response': None,
                'confidence': similarity_score,
                'matched_question': matched_question_text or matched_document
            }
    
    async def _evaluate_response_with_chatgpt(self, user_message: str, matched_question: str, 
                                            matched_answer: str, language: str, conversation_history: list = None, journey_id: str = None) -> Dict[str, Any]:
        """
        Ask ChatGPT to evaluate if the response is good and appropriate
        """
        
        # First check if the user message and matched answer are in the same language
        user_language = language_handler.detect_language(user_message)
        answer_language = language_handler.detect_language(matched_answer)
        
        # If languages don't match, skip the response
        if user_language != answer_language:
            print(f"üåê Language mismatch: user={user_language}, answer={answer_language} - skipping response")
            return {'action': 'skip'}
        
        # Format conversation history for context
        conversation_context = ""
        if conversation_history:
            # Get the latest 3 messages for context
            recent_messages = conversation_history[-3:] if len(conversation_history) >= 3 else conversation_history
            
            if language == 'ar':
                conversation_context = "\n\nÿ≥ŸäÿßŸÇ ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ© (ÿ¢ÿÆÿ± 3 ÿ±ÿ≥ÿßÿ¶ŸÑ):\n"
                for i, msg in enumerate(recent_messages, 1):
                    role = "ÿßŸÑÿπŸÖŸäŸÑ" if msg.get('role') == 'user' else "ÿßŸÑŸàŸÉŸäŸÑ"
                    conversation_context += f"{i}. {role}: {msg.get('content', '')}\n"
            else:
                conversation_context = "\n\nConversation context (last 3 messages):\n"
                for i, msg in enumerate(recent_messages, 1):
                    role = "Customer" if msg.get('role') == 'user' else "Agent"
                    conversation_context += f"{i}. {role}: {msg.get('content', '')}\n"
        
        if language == 'ar':
            evaluation_prompt = f"""ÿ£ŸÜÿ™ ŸÖŸÇŸäŸÖ ÿµÿßÿ±ŸÖ ÿ¨ÿØÿßŸã ŸÑÿ¨ŸàÿØÿ© ÿßŸÑÿ±ÿØŸàÿØ ŸÅŸä ÿÆÿØŸÖÿ© ÿßŸÑÿπŸÖŸÑÿßÿ° ŸÑÿ¥ÿ±ŸÉÿ© ÿ£ÿ®ÿßÿ± ŸÑÿ™ŸàÿµŸäŸÑ ÿßŸÑŸÖŸäÿßŸá. 

ŸÖŸáŸÖÿ™ŸÉ ÿßŸÑŸàÿ≠ŸäÿØÿ©: ÿ™ÿ≠ÿØŸäÿØ ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿ±ÿ≥ÿßŸÑÿ© ÿßŸÑÿπŸÖŸäŸÑ ÿ™ÿ≠Ÿäÿ© ÿ£Ÿà ÿ¥ŸÉÿ± ÿ≠ŸÇŸäŸÇŸä ŸÅŸÇÿ∑.

- ÿ±ÿ≥ÿßŸÑÿ© ÿßŸÑÿπŸÖŸäŸÑ ÿßŸÑÿ≠ÿßŸÑŸäÿ©: "{user_message}"
- ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿßŸÑŸÖÿ¥ÿßÿ®Ÿá ŸÖŸÜ ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™: "{matched_question}"
- ÿßŸÑÿ±ÿØ ÿßŸÑŸÖÿ≠ŸÅŸàÿ∏: "{matched_answer}"
{conversation_context}

ŸÇŸàÿßÿπÿØ ÿµÿßÿ±ŸÖÿ©:
- "reply": ŸÅŸÇÿ∑ ŸÑŸÑÿ™ÿ≠Ÿäÿßÿ™ ÿßŸÑÿ≠ŸÇŸäŸÇŸäÿ© ÿßŸÑÿ®ÿ≥Ÿäÿ∑ÿ©: (ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖÿå ŸÖÿ±ÿ≠ÿ®ÿßÿå ÿ£ŸáŸÑÿßÿå ŸÖÿ≥ÿßÿ° ÿßŸÑÿÆŸäÿ±ÿå ÿµÿ®ÿßÿ≠ ÿßŸÑÿÆŸäÿ±)
- "reply": ŸÅŸÇÿ∑ ŸÑŸÑÿ¥ŸÉÿ± ÿßŸÑŸÖÿ®ÿßÿ¥ÿ± ÿßŸÑÿ®ÿ≥Ÿäÿ∑: (ÿ¥ŸÉÿ±ÿßŸãÿå Ÿäÿπÿ∑ŸäŸÉ ÿßŸÑÿπÿßŸÅŸäÿ©ÿå ÿßŸÑŸÑŸá ŸäŸàŸÅŸÇŸÉŸÖÿå ÿ¨ÿ≤ÿßŸÉ ÿßŸÑŸÑŸá ÿÆŸäÿ±)
- "skip": ŸÑÿ£Ÿä ÿ±ÿ≥ÿßŸÑÿ© ÿ£ÿÆÿ±Ÿâ ŸÖŸáŸÖÿß ŸÉÿßŸÜÿ™ ŸÖÿ§ÿØÿ®ÿ© ÿ£Ÿà ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿ™ÿ≠Ÿäÿ©

ÿ£ŸÖÿ´ŸÑÿ© ŸÑŸÑÿ±ÿ≥ÿßÿ¶ŸÑ ÿßŸÑÿ™Ÿä ŸÑÿß ÿ™Ÿèÿπÿ™ÿ®ÿ± ÿ™ÿ≠Ÿäÿ© ÿ£Ÿà ÿ¥ŸÉÿ±:
- "ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖÿå ÿπŸÜÿØŸä ÿßÿ≥ÿ™ŸÅÿ≥ÿßÿ±" ‚Üí continue (Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿ≥ÿ§ÿßŸÑ)
- "ÿ¥ŸÉÿ±ÿßŸã ŸÑŸÉÿå ÿ®ÿ≥ ÿπŸÜÿØŸä ÿ≥ÿ§ÿßŸÑ" ‚Üí continue (Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿ≥ÿ§ÿßŸÑ)
- "ÿ£ÿ®Ÿä ÿ£ÿ∑ŸÑÿ® ŸÖŸäÿßŸá" ‚Üí continue (ÿ∑ŸÑÿ® ÿÆÿØŸÖÿ©)
- "ŸÖŸÖŸÉŸÜ ÿ™ÿ≥ÿßÿπÿØŸÜŸäÿü" ‚Üí continue (ÿ∑ŸÑÿ® ŸÖÿ≥ÿßÿπÿØÿ©)
- "ŸÉŸäŸÅ ÿ£ŸÇÿØÿ± ÿ£ÿ∑ŸÑÿ®ÿü" ‚Üí continue (ÿ≥ÿ§ÿßŸÑ)

ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿßŸÑÿ±ÿ≥ÿßŸÑÿ© ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿ£Ÿä ÿ¥Ÿäÿ° ÿ∫Ÿäÿ± ÿßŸÑÿ™ÿ≠Ÿäÿ© ÿ£Ÿà ÿßŸÑÿ¥ŸÉÿ± ŸÅŸÇÿ∑ÿå ÿßÿÆÿ™ÿ± "continue".

ÿßÿÆÿ™ÿ± Ÿàÿßÿ≠ÿØ ŸÅŸÇÿ∑: reply ÿ£Ÿà skip ÿ£Ÿà continue"""
        else:
            evaluation_prompt = f"""You are a very strict response quality evaluator for Abar water delivery customer service.

Your only task: Determine if the customer message is ONLY a greeting or thanks.

- Current customer message: "{user_message}"
- Similar question from database: "{matched_question}"
- Stored response: "{matched_answer}"
{conversation_context}

Strict rules:
- "reply": Only for simple genuine greetings: (Hello, Hi, Good morning, Good evening, Peace be upon you)
- "reply": Only for simple direct thanks: (Thank you, Thanks, God bless you, I appreciate it)
- "skip": For any other message no matter how polite or containing greetings

Examples of messages that are NOT greetings or thanks:
- "Hello, I have a question" ‚Üí continue (contains question)
- "Thank you, but I need help" ‚Üí continue (contains request)
- "I want to order water" ‚Üí continue (service request)
- "Can you help me?" ‚Üí continue (request for help)
- "How can I order?" ‚Üí continue (question)

If the message contains anything other than ONLY greeting or thanks, choose "continue".

Choose only one: reply or skip or continue"""
        
        try:
            print(f"ü§ñ ChatGPT evaluation prompt: {evaluation_prompt}")
            
            # Build the complete messages for the API call
            system_content = """
                        You are an extremely strict evaluator for customer service response quality at Abar Water Delivery.

                        Your ONLY task: Determine if the customer message is PURELY a greeting or thanks with NO additional content.

                        Rules:
                        - reply: ONLY if the message is a simple standalone greeting (e.g. ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ, ŸÖÿ±ÿ≠ÿ®ÿß, ÿ£ŸáŸÑÿß) or simple direct thanks (e.g. ÿ¥ŸÉÿ±ÿßŸã, Ÿäÿπÿ∑ŸäŸÉ ÿßŸÑÿπÿßŸÅŸäÿ©, ÿßŸÑŸÑŸá ŸäŸàŸÅŸÇŸÉŸÖ), with ABSOLUTELY NO other content.
                        - skip: If the message needs no response (e.g. ÿ£ŸàŸÉŸä, ÿ™ŸÖÿßŸÖ, ŸÜÿπŸÖ).
                        - continue: If the message includes ANY question, request, scheduling, or information ‚Äî even if it starts with greetings or thanks.

                        Critical examples of messages that are NOT greetings/thanks:
                        - "ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖÿå ÿπŸÜÿØŸä ÿßÿ≥ÿ™ŸÅÿ≥ÿßÿ±" ‚Üí continue (contains question)
                        - "ÿ¥ŸÉÿ±ÿßŸã ŸÑŸÉÿå ÿ®ÿ≥ ÿπŸÜÿØŸä ÿ≥ÿ§ÿßŸÑ" ‚Üí continue (contains question)
                        - "ÿ£ÿ®Ÿä ÿ£ÿ∑ŸÑÿ® ŸÖŸäÿßŸá" ‚Üí continue (service request)  
                        - "ŸÖŸÖŸÉŸÜ ÿ™ÿ≥ÿßÿπÿØŸÜŸäÿü" ‚Üí continue (request for help)
                        - "ŸÉŸäŸÅ ÿ£ŸÇÿØÿ± ÿ£ÿ∑ŸÑÿ®ÿü" ‚Üí continue (question)
                        - "ŸäŸÖÿØŸä ÿ™ŸàÿµŸÑŸàŸÜŸá ÿßŸÑŸäŸàŸÖ ÿßŸÉŸàŸÜ ÿ¥ÿßŸÉÿ± ŸÑŸÉŸÖ" ‚Üí continue (question with thanks)
                        - "ÿßÿ®ŸäŸá ÿßŸÑŸÑŸäŸÑÿ©" ‚Üí continue (request)
                        - "ŸÖŸàÿπÿØŸÜÿß ÿ®ŸÉÿ±Ÿá ÿ®ÿßÿ∞ŸÜ ÿßŸÑŸÑŸá" ‚Üí continue (informational statement)

                        Only pure greetings/thanks are allowed:
                        - "ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ" ‚Üí reply
                        - "ÿ¥ŸÉÿ±ÿßŸã" ‚Üí reply
                        - "Ÿäÿπÿ∑ŸäŸÉ ÿßŸÑÿπÿßŸÅŸäÿ©" ‚Üí reply
                        - "ŸÖÿ±ÿ≠ÿ®ÿß" ‚Üí reply

                        Final instruction:
                        Be extremely conservative ‚Äî choose `reply` ONLY if you are 100% certain it's PURELY a greeting or thanks with NO other content.
                        
                        """
            
            messages = [
                {"role": "system", "content": system_content},
                {"role": "user", "content": evaluation_prompt}
            ]
            
            # Log the complete prompt before sending to LLM
            if LOGGING_AVAILABLE and journey_id:
                complete_prompt = f"SYSTEM: {system_content}\n\nUSER: {evaluation_prompt}"
                message_journey_logger.add_step(
                    journey_id=journey_id,
                    step_type="embedding_llm_evaluation_prompt",
                    description="Sending evaluation prompt to ChatGPT for embedding response validation",
                    data={
                        "complete_prompt": complete_prompt,
                        "system_prompt": system_content,
                        "user_prompt": evaluation_prompt,
                        "user_message": user_message,
                        "matched_question": matched_question,
                        "matched_answer": matched_answer,
                        "model": "gpt-4o-mini"
                    }
                )
            
            # Time the LLM call
            llm_start_time = time.time()
            
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                max_tokens=20,
                temperature=0.1
            )
            
            llm_duration = int((time.time() - llm_start_time) * 1000)
            evaluation = response.choices[0].message.content.strip().lower()
            
            # Log the complete response from LLM
            if LOGGING_AVAILABLE and journey_id:
                message_journey_logger.log_llm_interaction(
                    journey_id=journey_id,
                    llm_type="openai",
                    prompt=f"SYSTEM: {system_content}\n\nUSER: {evaluation_prompt}",
                    response=evaluation,
                    model="gpt-4o-mini",
                    duration_ms=llm_duration,
                    tokens_used={"total_tokens": response.usage.total_tokens if response.usage else None}
                )
                
                message_journey_logger.add_step(
                    journey_id=journey_id,
                    step_type="embedding_llm_evaluation_result",
                    description=f"ChatGPT evaluation completed: {evaluation}",
                    data={
                        "evaluation_result": evaluation,
                        "raw_response": response.choices[0].message.content,
                        "duration_ms": llm_duration,
                        "tokens_used": response.usage.total_tokens if response.usage else None,
                        "user_message": user_message,
                        "matched_question": matched_question,
                        "matched_answer": matched_answer
                    }
                )
            
            # Log the evaluation for debugging
            print(f"ü§ñ ChatGPT evaluation result: '{evaluation}'")
            
            # Map the response to our action format
            if 'reply' in evaluation:
                return {'action': 'reply'}
            elif 'skip' in evaluation:
                return {'action': 'skip'}
            elif 'continue' in evaluation:
                return {'action': 'continue'}
            else:
                # Default to continue if we can't parse the response
                print(f"‚ö†Ô∏è Could not parse evaluation result, defaulting to continue")
                return {'action': 'continue'}
                
        except Exception as e:
            print(f"‚ùå EmbeddingAgent: Error evaluating response with ChatGPT: {str(e)}")
            # Default to continue on error
            return {'action': 'continue'}

    async def debug_knowledge_base_structure(self, sample_size: int = 5) -> Dict[str, Any]:
        """
        Debug method to check the knowledge base structure and verify question-answer linking
        """
        print(f"üîç DEBUG: Testing knowledge base structure...")
        
        try:
            # Get a sample of documents from the knowledge base
            from vectorstore.chroma_db import chroma_manager
            
            # Get all documents
            all_data = chroma_manager.get_collection_safe().get(include=["documents", "metadatas", "embeddings"])
            
            if not all_data or not all_data['documents']:
                print(f"‚ùå DEBUG: No documents found in knowledge base")
                return {"status": "error", "message": "No documents found"}
            
            documents = all_data['documents']
            metadatas = all_data['metadatas']
            ids = all_data['ids']
            
            print(f"üìä DEBUG: Found {len(documents)} total documents")
            
            # Count questions and answers
            questions = []
            answers = []
            
            for i, (doc, metadata, doc_id) in enumerate(zip(documents, metadatas, ids)):
                if metadata.get('type') == 'question':
                    questions.append({'doc': doc, 'metadata': metadata, 'id': doc_id})
                else:
                    answers.append({'doc': doc, 'metadata': metadata, 'id': doc_id})
            
            print(f"üìà DEBUG: Found {len(questions)} questions and {len(answers)} answers")
            
            # Test a few question-answer pairs
            test_results = []
            
            for i, question_info in enumerate(questions[:sample_size]):
                print(f"\nüîç DEBUG Test {i+1}:")
                print(f"   Question: {question_info['doc'][:100]}...")
                print(f"   Question ID: {question_info['id']}")
                print(f"   Question Metadata: {question_info['metadata']}")
                
                answer_text = question_info['metadata'].get('answer_text', '')
                if answer_text and answer_text.strip():
                    print(f"   ‚úÖ Found Answer in metadata: {answer_text[:100]}...")
                    
                    test_results.append({
                        "question": question_info['doc'][:100],
                        "answer": answer_text[:100],
                        "status": "success"
                    })
                else:
                    print(f"   ‚ùå No answer found in metadata")
                    test_results.append({
                        "question": question_info['doc'][:100],
                        "answer": None,
                        "status": "no_answer_in_metadata"
                    })
            
            return {
                "status": "success",
                "total_documents": len(documents),
                "questions_count": len(questions),
                "answers_count": len(answers),
                "test_results": test_results
            }
            
        except Exception as e:
            print(f"‚ùå DEBUG: Error testing knowledge base structure: {str(e)}")
            return {"status": "error", "message": str(e)}

# Create and export the instance
embedding_agent = EmbeddingAgent() 